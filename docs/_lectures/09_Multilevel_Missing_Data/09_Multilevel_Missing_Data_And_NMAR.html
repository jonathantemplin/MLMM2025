<!DOCTYPE html>
<html lang="en"><head>
<script src="09_Multilevel_Missing_Data_And_NMAR_files/libs/clipboard/clipboard.min.js"></script>
<script src="09_Multilevel_Missing_Data_And_NMAR_files/libs/quarto-html/tabby.min.js"></script>
<script src="09_Multilevel_Missing_Data_And_NMAR_files/libs/quarto-html/popper.min.js"></script>
<script src="09_Multilevel_Missing_Data_And_NMAR_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="09_Multilevel_Missing_Data_And_NMAR_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="09_Multilevel_Missing_Data_And_NMAR_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="09_Multilevel_Missing_Data_And_NMAR_files/libs/quarto-html/quarto-syntax-highlighting-f23ab18612d661d7bd56dbc0c0fd3817.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.34">

  <meta name="author" content="Lecture 9: April 30, 2025">
  <title>Multilevel Missing Data</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="09_Multilevel_Missing_Data_And_NMAR_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="09_Multilevel_Missing_Data_And_NMAR_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="09_Multilevel_Missing_Data_And_NMAR_files/libs/revealjs/dist/theme/quarto-16232f29d62c098d89a38a65e10d436d.css">
  <link href="09_Multilevel_Missing_Data_And_NMAR_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="09_Multilevel_Missing_Data_And_NMAR_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="09_Multilevel_Missing_Data_And_NMAR_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="09_Multilevel_Missing_Data_And_NMAR_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Multilevel Missing Data</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Lecture 9: April 30, 2025 
</div>
</div>
</div>

</section>
<section id="section-8.1-chapter-overview" class="slide level2">
<h2>Section 8.1: Chapter Overview</h2>
<p>This chapter focuses on handling missing data in <strong>multilevel data sets</strong>.</p>
</section>
<section id="what-is-multilevel-data" class="slide level2">
<h2>What is Multilevel Data?</h2>
<ul>
<li><strong>Hierarchically Structured:</strong> Observations are nested within higher-level organizational units</li>
<li><strong>Ubiquitous:</strong> Found in many disciplines</li>
<li><strong>Nesting:</strong> Can involve two or more levels (e.g., Level 1 nested in Level 2, or Level 1 in Level 2 in Level 3)</li>
</ul>
</section>
<section id="examples-of-multilevel-data" class="slide level2">
<h2>Examples of Multilevel Data</h2>
<ul>
<li>Repeated measurements nested within <strong>persons</strong></li>
<li>Students grouped in <strong>classrooms</strong> or <strong>schools</strong></li>
<li>Romantic partners paired within <strong>dyads</strong></li>
<li>Employees nested within <strong>organizations</strong></li>
<li>Survey respondents grouped in <strong>geographical regions</strong></li>
<li>Clients clustered within <strong>therapists</strong></li>
<li><em>Three-level example:</em> Repeated measurements nested within students, students nested in schools</li>
</ul>
</section>
<section id="data-analysis-focus" class="slide level2">
<h2>Data Analysis Focus</h2>
<ul>
<li><strong>Multilevel Regression Models:</strong> Specifically models with random effects
<ul>
<li>These are very common tools for analyzing hierarchical data</li>
</ul></li>
<li><strong>Recommended Resources:</strong>
<ul>
<li>Gelman &amp; Hill (2007)</li>
<li>Hoffman (2015)</li>
<li>Hox, Moerbeek, &amp; Van de Schoot (2017)</li>
<li>Raudenbush &amp; Bryk (2002)</li>
<li>Snijders &amp; Bosker (2012)</li>
<li>Verbeke &amp; Molenberghs (2000)</li>
</ul></li>
</ul>
</section>
<section id="missing-data-handling-in-multilevel-models" class="slide level2">
<h2>Missing Data Handling in Multilevel Models</h2>
<ul>
<li><strong>Recent Development:</strong> Methods specifically for multilevel missing data are relatively new and important.</li>
<li><strong>Chapter Focus:</strong>
<ul>
<li>Bayesian Estimation</li>
<li>Model-Based Imputation (often more adept than ML)</li>
<li>Joint Model Imputation</li>
<li>Fully Conditional Specification (FCS / MICE)</li>
<li>Maximum Likelihood (ML)</li>
</ul></li>
<li><strong>Core Idea:</strong> Often involves MCMC estimating factored regressions (focal + supporting models) to create distributions for missing values. Imputations = Predictions + Noise (from a multilevel model)</li>
</ul>
</section>
<section id="section-8.2-random-intercept-regression-models" class="slide level2">
<h2>Section 8.2: Random Intercept Regression Models</h2>
</section>
<section id="example-math-problem-solving-study" class="slide level2">
<h2>Example: Math Problem-Solving Study</h2>
<ul>
<li><strong>Data:</strong> Educational experiment data (from companion website)</li>
<li><strong>Structure:</strong> 2-Level Hierarchy
<ul>
<li>Level 1: Students (<span class="math inline">\(n_j \approx 34\)</span> per school)</li>
<li>Level 2: Schools (<span class="math inline">\(J=29\)</span>)</li>
</ul></li>
<li><strong>Design:</strong> Schools randomly assigned to experimental (new curriculum) or comparison (standard curriculum) condition</li>
<li><strong>Outcome (Y):</strong> End-of-year math problem-solving assessment (IRT scores, 37-65)</li>
</ul>
</section>
<section id="key-feature-multilevel-variation" class="slide level2">
<h2>Key Feature: Multilevel Variation</h2>
<ul>
<li>Variation and covariation exist at <strong>both levels</strong>
<ul>
<li>Student scores vary <em>within</em> schools</li>
<li>School average scores vary <em>between</em> schools</li>
</ul></li>
<li><strong>Intraclass Correlation (ICC):</strong> Proportion of total variance at Level 2
<ul>
<li>Example: ICC <span class="math inline">\(\approx 0.26\)</span> for problem-solving scores means ~26% of variance is between schools</li>
</ul></li>
<li><strong>Predictors:</strong> Level-1 predictors (like student math scores) can also have within- and between-group variation. Missing data methods need to preserve this.</li>
</ul>
</section>
<section id="random-intercept-model-concept" class="slide level2">
<h2>Random Intercept Model: Concept</h2>
<ul>
<li><strong>Definition:</strong> A regression model where the intercept coefficient (<span class="math inline">\(\beta_{0j}\)</span>) is allowed to vary randomly across Level-2 units (groups/clusters)</li>
<li><strong>Suitability:</strong> Amenable to various missing data handling methods (agnostic imputation, ML)</li>
<li><strong>Example Predictors:</strong>
<ul>
<li>Level 1: Standardized math scores (<code>STANMATH</code>)</li>
<li>Level 2: Teacher experience (<code>TEACHEXP</code>)</li>
</ul></li>
</ul>
</section>
<section id="level-1-model-within-cluster" class="slide level2">
<h2>Level-1 Model (Within-Cluster)</h2>
<p>Describes variation <em>among students within the same school</em></p>
<p><span class="math display">\[
Y_{ij} = \beta_{0j} + \beta_{1}X_{1ij} + \epsilon_{ij}
\]</span></p>
<ul>
<li><span class="math inline">\(Y_{ij}\)</span>: Outcome for student <span class="math inline">\(i\)</span> in school <span class="math inline">\(j\)</span></li>
<li><span class="math inline">\(X_{1ij}\)</span>: Level-1 predictor value for student <span class="math inline">\(i\)</span> in school <span class="math inline">\(j\)</span></li>
<li><span class="math inline">\(\beta_{0j}\)</span>: <strong>Random intercept</strong> for school <span class="math inline">\(j\)</span> (Varies across schools)</li>
<li><span class="math inline">\(\beta_{1}\)</span>: Common (fixed) slope for <span class="math inline">\(X_1\)</span> (Same across schools)</li>
<li><span class="math inline">\(\epsilon_{ij}\)</span>: Level-1 residual (within-school error) for student <span class="math inline">\(i\)</span>
<ul>
<li>Assumed <span class="math inline">\(\epsilon_{ij} \sim N(0, \sigma_{\epsilon}^2)\)</span></li>
</ul></li>
</ul>
</section>
<section id="level-2-model-between-cluster" class="slide level2">
<h2>Level-2 Model (Between-Cluster)</h2>
<p>Models the variation in the school-specific intercepts (<span class="math inline">\(\beta_{0j}\)</span>).</p>
<p><span class="math display">\[
\beta_{0j} = \beta_{0} + \beta_{2}X_{2j} + b_{0j}
\]</span></p>
<ul>
<li><span class="math inline">\(\beta_{0j}\)</span>: The random intercept (outcome in this model)</li>
<li><span class="math inline">\(\beta_{0}\)</span>: Grand mean intercept (average intercept across all schools)</li>
<li><span class="math inline">\(X_{2j}\)</span>: Level-2 predictor for school <span class="math inline">\(j\)</span></li>
<li><span class="math inline">\(\beta_{2}\)</span>: Effect of Level-2 predictor <span class="math inline">\(X_2\)</span> on the intercept</li>
<li><span class="math inline">\(b_{0j}\)</span>: Level-2 residual (random effect) for school <span class="math inline">\(j\)</span>. Captures unexplained variation in intercepts.
<ul>
<li>Assumed <span class="math inline">\(b_{0j} \sim N(0, \sigma_{b_0}^2)\)</span></li>
</ul></li>
</ul>
</section>
<section id="combined-model-single-equation" class="slide level2">
<h2>Combined Model (Single Equation)</h2>
<p>Substitute the Level-2 equation into the Level-1 equation:</p>
<p><span class="math display">\[
Y_{ij} = (\beta_{0} + b_{0j}) + \beta_{1}X_{1ij} + \beta_{2}X_{2j} + \epsilon_{ij}
\]</span></p>
<p>Alternatively:</p>
<p><span class="math display">\[
Y_{ij} = E(Y_{ij}|X_{1ij}, X_{2j}) + \epsilon_{ij}
\]</span></p>
<ul>
<li>Assumes <span class="math inline">\(Y_{ij} \sim N(E(Y_{ij}|X_{1ij}, X_{2j}), \sigma_{\epsilon}^2)\)</span></li>
<li>Interpretation: Outcome scores <span class="math inline">\(Y_{ij}\)</span> are normally distributed around the predicted values derived from their specific school’s regression line</li>
</ul>
</section>
<section id="factored-regression-specification" class="slide level2">
<h2>Factored Regression Specification</h2>
<ul>
<li><strong>Concept:</strong> Expresses the joint distribution of all variables (<span class="math inline">\(Y, X_1, X_2\)</span>) as a product of simpler conditional distributions using the probability chain rule. Essential for handling missing predictors</li>
<li><strong>Two Main Approaches:</strong>
<ol type="1">
<li>Sequential Specification</li>
<li>Partially Factored Specification</li>
</ol></li>
</ul>
</section>
<section id="sequential-specification" class="slide level2">
<h2>1. Sequential Specification</h2>
<p>Factorizes the joint distribution into a product of univariate conditional distributions:</p>
<p><span class="math display">\[
f(Y, X_1, X_2) = f(Y | X_1, X_2) \times f(X_1 | X_2) \times f(X_2)
\]</span></p>
<ul>
<li><span class="math inline">\(f(Y | X_1, X_2)\)</span>: The focal analysis model</li>
<li><span class="math inline">\(f(X_1 | X_2)\)</span>: A model for the Level-1 predictor, conditional on Level-2 predictor(s)</li>
<li><span class="math inline">\(f(X_2)\)</span>: A model for the Level-2 predictor</li>
<li><strong>Important Ordering:</strong> Lower-level variables condition on higher-level variables</li>
</ul>
</section>
<section id="sequential-predictor-models" class="slide level2">
<h2>Sequential: Predictor Models</h2>
<ul>
<li><strong>Level-1 Predictor Model:</strong> A random intercept model for <span class="math inline">\(X_1\)</span> <span class="math display">\[ X_{1ij} = (\gamma_{01} + g_{01j}) + \gamma_{11}X_{2j} + r_{1ij} \]</span>
<ul>
<li><span class="math inline">\(\gamma\)</span>: Regression coefficients</li>
<li><span class="math inline">\(g_{01j}\)</span>: Random intercept residual for <span class="math inline">\(X_1 \sim N(0, \sigma_{g_{01}}^2)\)</span>. Captures between-school variation in average <span class="math inline">\(X_1\)</span></li>
<li><span class="math inline">\(r_{1ij}\)</span>: Within-cluster residual for <span class="math inline">\(X_1 \sim N(0, \sigma_{r_1}^2)\)</span>. Captures within-school variation in <span class="math inline">\(X_1\)</span></li>
</ul></li>
<li><strong>Level-2 Predictor Model:</strong> An “empty” single-level model for <span class="math inline">\(X_2\)</span> (should be corresponding level-2 mean for level-1 variable) <span class="math display">\[ X_{2j} = \gamma_{02} + r_{2j} \]</span>
<ul>
<li><span class="math inline">\(\gamma_{02}\)</span>: Grand mean of <span class="math inline">\(X_2\)</span></li>
<li><span class="math inline">\(r_{2j}\)</span>: Between-cluster residual for <span class="math inline">\(X_2 \sim N(0, \sigma_{r_2}^2)\)</span></li>
</ul></li>
</ul>
</section>
<section id="partially-factored-specification" class="slide level2">
<h2>2. Partially Factored Specification</h2>
<p>Factorizes into the focal model and a joint multivariate distribution for predictors:</p>
<p><span class="math display">\[
f(Y, X_1, X_2) = f(Y | X_1, X_2) \times f(X_1, X_2)
\]</span></p>
<ul>
<li><span class="math inline">\(f(X_1, X_2)\)</span>: A two-part (Level-1 and Level-2) multivariate normal distribution</li>
<li><strong>Decomposition:</strong> Splits L1 predictor <span class="math inline">\(X_1\)</span> into components: <span class="math display">\[ X_{1ij} = \underbrace{\mu_{1}}_{\text{Grand Mean}} + \underbrace{(\mu_{1j} - \mu_{1})}_{\text{Between Cluster Dev.}} + \underbrace{(X_{1ij} - \mu_{1j})}_{\text{Within Cluster Dev.}} \]</span>
<ul>
<li><span class="math inline">\(\mu_{1j}\)</span> is the <strong>latent</strong> group mean for cluster <span class="math inline">\(j\)</span></li>
</ul></li>
</ul>
</section>
<section id="partially-factored-predictor-models" class="slide level2">
<h2>Partially Factored: Predictor Models</h2>
<ul>
<li><strong>Within-Cluster Model for <span class="math inline">\(X_1\)</span>:</strong> <span class="math inline">\(X_1\)</span> scores as deviations around “latent” (unestimated) group means <span class="math display">\[ X_{1ij} = \mu_{1j} + r_{1ij(W)} \quad \text{where} \quad X_{1ij} \sim N(\mu_{1j}, \sigma_{r_{1(W)}}^2) \]</span>
<ul>
<li><span class="math inline">\(r_{1ij(W)}\)</span>: Within-cluster residual</li>
</ul></li>
<li><strong>Between-Cluster Model for <span class="math inline">\((\mu_{1j}, X_{2j})\)</span>:</strong> Latent means (<span class="math inline">\(\mu_{1j}\)</span>) and L2 scores (<span class="math inline">\(X_{2j}\)</span>) are bivariate normal. <span class="math display">\[ \begin{pmatrix} \mu_{1j} \\ X_{2j} \end{pmatrix} = \begin{pmatrix} \mu_{1} \\ \mu_{2} \end{pmatrix} + \begin{pmatrix} r_{1j(B)} \\ r_{2j(B)} \end{pmatrix} \quad \text{where} \quad \begin{pmatrix} r_{1j(B)} \\ r_{2j(B)} \end{pmatrix} \sim N_2 \left( \mathbf{0}, \Sigma_{(B)} \right) \]</span>
<ul>
<li><span class="math inline">\(\Sigma_{(B)}\)</span> is the <span class="math inline">\(2 \times 2\)</span> between-cluster covariance matrix</li>
</ul></li>
<li><strong>Note:</strong> Can also be parameterized via round-robin regressions. Partially factored is ideal for models with centered predictors</li>
</ul>
</section>
<section id="distribution-of-missing-values-outcome-y" class="slide level2">
<h2>Distribution of Missing Values: Outcome <span class="math inline">\(Y\)</span></h2>
<ul>
<li>Defined solely by the <strong>focal analysis model</strong> (Combined Model)</li>
<li>The posterior predictive distribution for <span class="math inline">\(Y_{ij(mis)}\)</span> is: <span class="math display">\[ Y_{ij(mis)} \sim N(E(Y_{ij}|X_{1ij}, X_{2j}), \sigma_{\epsilon}^2) \]</span></li>
<li><strong>Imputation:</strong> Draw a random value from this normal distribution
<ul>
<li>The mean <span class="math inline">\(E(Y_{ij}|...)\)</span> depends on the specific cluster <span class="math inline">\(j\)</span>’s intercept (<span class="math inline">\(\beta_{0j}\)</span>)</li>
<li>The variance <span class="math inline">\(\sigma_{\epsilon}^2\)</span> is the within-cluster residual variance</li>
</ul></li>
</ul>
</section>
<section id="distribution-of-missing-values-predictor-x_1" class="slide level2">
<h2>Distribution of Missing Values: Predictor <span class="math inline">\(X_1\)</span></h2>
<ul>
<li>Drawn from the conditional distribution <span class="math inline">\(f(X_1 | Y, X_2)\)</span></li>
<li>This distribution is proportional to the product of the focal model and the predictor model for <span class="math inline">\(X_1\)</span>: <span class="math display">\[ f(X_1 | Y, X_2) \propto f(Y | X_1, X_2) \times f(X_1 | X_2) \]</span></li>
<li>Combining the kernels yields a normal distribution: <span class="math display">\[ f(X_{1ij(mis)} | Y_{ij}, X_{2j}) = N(E(X_{1ij}|...), Var(X_{1ij}|...)) \]</span></li>
<li><strong>Mean and Variance:</strong> Depend on parameters from <em>both</em> the focal model (<span class="math inline">\(\beta\)</span>’s, <span class="math inline">\(\sigma_{\epsilon}^2\)</span>) and the predictor model (<span class="math inline">\(\mu_{1j}\)</span>, <span class="math inline">\(\sigma_{r_{1(W)}}^2\)</span>). Includes random effects (<span class="math inline">\(\beta_{0j}, \mu_{1j}\)</span>)</li>
</ul>
</section>
<section id="distribution-of-missing-values-predictor-x_2" class="slide level2">
<h2>Distribution of Missing Values: Predictor <span class="math inline">\(X_2\)</span></h2>
<ul>
<li>Drawn from the conditional distribution <span class="math inline">\(f(X_2 | Y, X_1)\)</span></li>
<li>Using the partially factored specification: <span class="math display">\[ f(X_2 | Y, X_1) \propto f(Y | X_1, X_2) \times f(X_2 | X_1) \]</span></li>
<li><span class="math inline">\(f(Y | X_1, X_2)\)</span> is from the focal model; <span class="math inline">\(f(X_2 | X_1)\)</span> is from the between-cluster predictor model</li>
<li><strong>Important:</strong> <span class="math inline">\(X_2\)</span> is constant within cluster <span class="math inline">\(j\)</span>. The focal model’s contribution is repeated <span class="math inline">\(n_j\)</span> times: <span class="math display">\[ f(X_{2j(mis)}|...) \propto \left[ \prod_{i=1}^{n_j} N(E(Y_{ij}|...), \sigma_{\epsilon}^2) \right] \times N(E(X_{2j}|\mu_{1j}), \sigma_{r_{2(B)}}^2) \]</span></li>
<li><strong>Imputation:</strong> Often requires sampling methods like Metropolis-Hastings due to complexity</li>
</ul>
</section>
<section id="mcmc-algorithm-overview" class="slide level2">
<h2>MCMC Algorithm: Overview</h2>
<ul>
<li><strong>Posterior Distribution:</strong> Complex function describing joint probability of parameters, random effects, latent means, and missing values given observed data</li>
<li><strong>Core Logic:</strong> Extends standard Bayesian MCMC
<ul>
<li>Estimate one unknown quantity at a time (parameter, latent variable, missing value)</li>
<li>Condition on the current values of all other quantities</li>
</ul></li>
<li>Full conditional distributions are available in the literature</li>
</ul>
</section>
<section id="mcmc-algorithm-generic-recipe" class="slide level2">
<h2>MCMC Algorithm: Generic Recipe</h2>
<ol type="1">
<li>Assign starting values (parameters, random effects, missing values)</li>
<li><strong>Do for <span class="math inline">\(t=1\)</span> to T iterations:</strong>
<ul>
<li>Estimate focal model’s parameters, given everything else</li>
<li>Estimate focal model’s random effects, given everything else</li>
<li>Estimate each predictor model’s parameters, given everything else</li>
<li>Estimate each predictor model’s random effects, given everything else</li>
<li>Impute dependent variable (<span class="math inline">\(Y_{mis}\)</span>) given focal model parameters</li>
<li>Impute each predictor (<span class="math inline">\(X_{mis}\)</span>) given focal and supporting models</li>
</ul></li>
<li>**Repeat*</li>
</ol>
</section>
<section id="analysis-example-setup-math-study" class="slide level2">
<h2>Analysis Example: Setup (Math Study)</h2>
<ul>
<li><strong>Model:</strong> Random intercept regression <span class="math display">\[ PROBSOLVE_{ij} = (\beta_{0} + b_{0j}) + \beta_{1}PRETEST_{ij} + \beta_{2}STANMATH_{ij} \]</span> <span class="math display">\[ + \beta_{3}FRLUNCH_{ij} + \beta_{4}TEACHEXP_{j} + \beta_{5}CONDITION_{j} + \epsilon_{ij} \]</span></li>
<li><strong>Predictors:</strong>
<ul>
<li>L1: PRETEST (complete), STANMATH (7.3% missing), FRLUNCH (binary, 4.7% missing)</li>
<li>L2: TEACHEXP (10.3% missing), CONDITION (complete)</li>
</ul></li>
<li><strong>Outcome:</strong> PROBSOLVE (20.5% missing)</li>
<li><strong>Goal:</strong> Estimate treatment effect (<span class="math inline">\(\beta_5\)</span>) controlling for covariates</li>
</ul>
<p>NOTE: Example analysis is likely incorrectly specified (no level-2 variables from corresponding level-1 variables likely means conflated effects)</p>
</section>
<section id="analysis-example-factored-regression-options" class="slide level2">
<h2>Analysis Example: Factored Regression Options</h2>
<ul>
<li><strong>Sequential:</strong> Product of univariate distributions. Order: Incomplete L1, Complete L1, Incomplete L2, Complete L2. Use latent response for binary predictors (FRLUNCH).</li>
<li><strong>Partially Factored:</strong> Multivariate normal for predictors (PRETEST, STANMATH, FRLUNCH<em>, TEACHEXP, CONDITION</em>)
<ul>
<li>Level 1 predictors decomposed</li>
<li>Within-cluster model: Correlated deviations around latent group means (<span class="math inline">\(\mu_j\)</span>)</li>
<li>Between-cluster model: Latent group means + L2 vars are multivariate normal</li>
<li>Preferred here due to centering</li>
</ul></li>
</ul>
</section>
<section id="random-coefficient-models" class="slide level2">
<h2>Random Coefficient Models</h2>
</section>
<section id="example-daily-diary-study-health-psych" class="slide level2">
<h2>Example: Daily Diary Study (Health Psych)</h2>
<ul>
<li><strong>Data:</strong> <span class="math inline">\(J=132\)</span> participants with chronic pain</li>
<li><strong>Structure:</strong> 2-Level Hierarchy (Repeated Measures)
<ul>
<li>Level 1: Daily assessments (up to <span class="math inline">\(n_j=21\)</span> days: mood, sleep, pain)</li>
<li>Level 2: Persons</li>
</ul></li>
<li><strong>Variables:</strong> L1 (daily), L2 (person-level demographics, psych variables like pain acceptance, catastrophizing)</li>
<li><strong>ICC Example:</strong> Positive Affect ICC <span class="math inline">\(\approx 0.63\)</span>. High between-person variation typical for repeated measures</li>
</ul>
</section>
<section id="random-coefficient-slope-model-concept" class="slide level2">
<h2>Random Coefficient (Slope) Model: Concept</h2>
<ul>
<li><strong>Definition:</strong> A multilevel regression where the influence (slope) of one or more Level-1 predictors varies across Level-2 units</li>
<li><strong>Example:</strong>
<ul>
<li>L1: Daily <code>PAIN</code> (<span class="math inline">\(X_1\)</span>) predicting daily <code>POSAFFECT</code> (<span class="math inline">\(Y\)</span>)</li>
<li>L2: Person-average <code>PAIN</code> (<span class="math inline">\(\mu_{1j}\)</span>) and <code>PAINACCEPT</code> (<span class="math inline">\(X_2\)</span>) predicting average <code>POSAFFECT</code></li>
<li><em>Key:</em> The effect of daily <code>PAIN</code> on <code>POSAFFECT</code> can differ from person to person</li>
</ul></li>
</ul>
</section>
<section id="level-1-model-within-person" class="slide level2">
<h2>Level-1 Model (Within-Person)</h2>
<p>Describes daily variation <em>within the same person</em>.</p>
<p><span class="math display">\[
Y_{ij} = \beta_{0j} + \beta_{1j}(X_{1ij} - \mu_{1j}) + \epsilon_{ij}
\]</span></p>
<ul>
<li><span class="math inline">\(Y_{ij}\)</span>: Positive affect for person <span class="math inline">\(j\)</span> on day <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(X_{1ij}\)</span>: Pain rating for person <span class="math inline">\(j\)</span> on day <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(\beta_{0j}\)</span>: <strong>Random intercept</strong> for person <span class="math inline">\(j\)</span> (person <span class="math inline">\(j\)</span>’s average affect)</li>
<li><span class="math inline">\(\beta_{1j}\)</span>: <strong>Random slope</strong> for person <span class="math inline">\(j\)</span> (person <span class="math inline">\(j\)</span>’s daily pain-affect association)</li>
<li><span class="math inline">\(\mu_{1j}\)</span>: <strong>Latent</strong> person-mean for <span class="math inline">\(X_1\)</span> (pain). <span class="math inline">\(X_1\)</span> is group-mean centered</li>
<li><span class="math inline">\(\epsilon_{ij}\)</span>: Level-1 residual (<span class="math inline">\(\sim N(0, \sigma_{\epsilon}^2)\)</span>)</li>
</ul>
<p>Interpretation: Regression lines vary in intercept <em>and</em> slope across persons</p>
</section>
<section id="level-2-model-between-person" class="slide level2">
<h2>Level-2 Model (Between-Person)</h2>
<p>Models variation in person-specific intercepts (<span class="math inline">\(\beta_{0j}\)</span>) and slopes (<span class="math inline">\(\beta_{1j}\)</span>)</p>
<p><span class="math display">\[
\beta_{0j} = \beta_{0} + \beta_{2}(\mu_{1j} - \mu_{1}) + \beta_{3}(X_{2j} - \mu_{2}) + b_{0j}
\]</span> <span class="math display">\[
\beta_{1j} = \beta_{1} + b_{1j}
\]</span></p>
<ul>
<li><span class="math inline">\(\beta_0, \beta_1\)</span>: Grand mean intercept &amp; slope</li>
<li><span class="math inline">\(\mu_{1j}\)</span>: Latent person-mean pain (predictor for <span class="math inline">\(\beta_{0j}\)</span>)</li>
<li><span class="math inline">\(X_{2j}\)</span>: Pain acceptance (predictor for <span class="math inline">\(\beta_{0j}\)</span>)</li>
<li><span class="math inline">\(\mu_1, \mu_2\)</span>: Grand means for predictors (used for centering)</li>
<li><span class="math inline">\(\beta_2, \beta_3\)</span>: Fixed effects of L2 predictors on intercept</li>
<li><span class="math inline">\(b_{0j}, b_{1j}\)</span>: L2 random effects (residuals for intercept &amp; slope). Assumed <span class="math inline">\(\begin{pmatrix} b_{0j} \\ b_{1j} \end{pmatrix} \sim N_2(\mathbf{0}, \Sigma_b)\)</span></li>
</ul>
</section>
<section id="combined-model-reduced-form" class="slide level2">
<h2>Combined Model (Reduced Form)</h2>
<p>Substitute L2 into L1:</p>
<p><span class="math display">\[
Y_{ij} = \underbrace{(\beta_{0} + \beta_{2}(\mu_{1j} - \mu_{1}) + \beta_{3}(X_{2j} - \mu_{2}) + b_{0j})}_{\beta_{0j}} + \underbrace{(\beta_{1} + b_{1j})}_{\beta_{1j}}(X_{1ij} - \mu_{1j}) + \epsilon_{ij}
\]</span></p>
<p>Alternatively: <span class="math inline">\(Y_{ij} = E(Y_{ij}|X_{1ij}, X_{2j}) + \epsilon_{ij}\)</span> where <span class="math inline">\(Y_{ij} \sim N(E(Y_{ij}|...), \sigma_{\epsilon}^2)\)</span></p>
<ul>
<li>This normal distribution defines <span class="math inline">\(P(Y_{ij(mis)}|...)\)</span></li>
</ul>
</section>
<section id="missing-data-challenge-nonlinearity" class="slide level2">
<h2>Missing Data Challenge: Nonlinearity</h2>
<ul>
<li>Random coefficient models feature a product term: <span class="math inline">\(X_{1ij} \times \beta_{1j}\)</span>
<ul>
<li>Level-1 predictor (<span class="math inline">\(X_1\)</span>) multiplied by a Level-2 latent variable/random effect (<span class="math inline">\(\beta_{1j}\)</span>)</li>
</ul></li>
<li><strong>Problem:</strong> Handling missingness when the L1 predictor (<span class="math inline">\(X_1\)</span>) involved in the random slope is incomplete is challenging
<ul>
<li>Some methods (e.g., current ML estimators) are prone to substantial bias</li>
</ul></li>
<li><strong>Solution:</strong> Bayesian estimation / Model-based MI using factored regression is effective</li>
</ul>
</section>
<section id="factored-regression-specification-1" class="slide level2">
<h2>Factored Regression Specification</h2>
<ul>
<li><strong>Partially Factored:</strong> Ideal due to centering <span class="math display">\[ f(Y | X_1, X_2) \times f(X_1, X_2) \]</span></li>
<li>The predictor model <span class="math inline">\(f(X_1, X_2)\)</span> is specified as before
<ul>
<li>The fact that the <em>focal</em> model <span class="math inline">\(f(Y|...)\)</span> has a random slope doesn’t change the <em>form</em> of the predictor model specification</li>
</ul></li>
</ul>
</section>
<section id="distribution-of-missing-values-outcome-y-1" class="slide level2">
<h2>Distribution of Missing Values: Outcome <span class="math inline">\(Y\)</span></h2>
<ul>
<li>Defined solely by the <strong>focal analysis model</strong></li>
<li>Posterior predictive distribution for <span class="math inline">\(Y_{ij(mis)}\)</span>: <span class="math display">\[ Y_{ij(mis)} \sim N(E(Y_{ij}|X_{1ij}, X_{2j}), \sigma_{\epsilon}^2) \]</span></li>
<li><strong>Imputation:</strong> Draw from this normal distribution
<ul>
<li>Mean <span class="math inline">\(E(Y_{ij}|...)\)</span> incorporates person <span class="math inline">\(j\)</span>’s specific intercept (<span class="math inline">\(\beta_{0j}\)</span>) AND slope (<span class="math inline">\(\beta_{1j}\)</span>)</li>
</ul></li>
</ul>
</section>
<section id="distribution-of-missing-values-predictor-x_1-1" class="slide level2">
<h2>Distribution of Missing Values: Predictor <span class="math inline">\(X_1\)</span></h2>
<ul>
<li>Drawn from conditional distribution <span class="math inline">\(f(X_1 | Y, X_2) \propto f(Y | X_1, X_2) \times f(X_1 | X_2)\)</span></li>
<li>Combining kernels yields a normal distribution: <span class="math display">\[ f(X_{1ij(mis)} | Y_{ij}, X_{2j}) = N(E(X_{1ij}|...), Var(X_{1ij}|...)) \]</span></li>
<li><span class="math inline">\(E(X_{1ij}|...) = Var(X_{1ij}|...) \times \left( \frac{\mu_{1j}}{\sigma_{r_{1(W)}}^2} + \frac{\beta_{1j}(Y_{ij} - (\beta_{0j} + \beta_{2}(\mu_{1j}-\mu_1) + ...))}{\sigma_{\epsilon}^2} \right)\)</span></li>
<li><span class="math inline">\(Var(X_{1ij}|...) = \left( \frac{1}{\sigma_{r_{1(W)}}^2} + \frac{\beta_{1j}^2}{\sigma_{\epsilon}^2} \right)^{-1}\)</span></li>
</ul>
</section>
<section id="key-issue-heteroscedasticity-in-x_1-imputations" class="slide level2">
<h2>Key Issue: Heteroscedasticity in <span class="math inline">\(X_1\)</span> Imputations</h2>
<ul>
<li>Look at the variance term for <span class="math inline">\(X_{1ij(mis)}\)</span>: <span class="math display">\[ Var(X_{1ij}|Y_{ij}, X_{2j}) = \left( \frac{1}{\sigma_{r_{1(W)}}^2} + \frac{\boldsymbol{\beta_{1j}^2}}{\sigma_{\epsilon}^2} \right)^{-1} \]</span></li>
<li>The variance (spread) of the imputation distribution depends on the <strong>person-specific random slope squared (<span class="math inline">\(\beta_{1j}^2\)</span>)</strong></li>
<li><strong>Implication:</strong> Imputations for <span class="math inline">\(X_1\)</span> should have different variances for different people. This is <strong>heteroscedastic</strong></li>
<li><strong>Problem:</strong> Methods assuming multivariate normality for imputation (e.g., standard FCS, current ML in SEM) cannot capture this heteroscedasticity easily and are prone to bias (especially underestimating slope variance)</li>
</ul>
</section>
<section id="analysis-example-setup-diary-study" class="slide level2">
<h2>Analysis Example: Setup (Diary Study)</h2>
<ul>
<li><strong>Model:</strong> Random coefficient model for POSAFFECT <span class="math display">\[ POSAFFECT_{ij} = \beta_{0j} + \beta_{1j}(PAIN_{ij} - \mu_{1j}) + \beta_{2}(SLEEP_{ij} - \mu_{2}) \]</span> <span class="math display">\[ + \beta_{3}(\mu_{1j} - \mu_{1}) + \beta_{4}(PAINACCEPT_{j} - \mu_{2}) + \beta_{5}FEMALE_{j} + \epsilon_{ij} \]</span></li>
<li><strong>Predictors:</strong>
<ul>
<li>L1: PAIN (random slope, latent group-mean centered), SLEEP (fixed slope, grand-mean centered)</li>
<li>L2: Mean PAIN (<span class="math inline">\(\mu_{1j}\)</span>), PAINACCEPT, FEMALE (binary)</li>
</ul></li>
<li><strong>Factored Regression:</strong> Partially factored, includes SLEEP, PAINACCEPT, FEMALE* in predictor model <span class="math inline">\(f(...)\)</span></li>
</ul>
</section>
<section id="section-8.4-multilevel-interaction-effects" class="slide level2">
<h2>Section 8.4: Multilevel Interaction Effects</h2>
</section>
<section id="example-employee-empowerment-study" class="slide level2">
<h2>Example: Employee Empowerment Study</h2>
<ul>
<li><strong>Data:</strong> <span class="math inline">\(N=630\)</span> employees from <span class="math inline">\(J=105\)</span> workgroups/teams (<span class="math inline">\(n_j=6\)</span>)</li>
<li><strong>Structure:</strong> 2-Level Hierarchy
<ul>
<li>Level 1: Employees</li>
<li>Level 2: Workgroups/Teams</li>
</ul></li>
<li><strong>Outcome:</strong> Employee Empowerment (<span class="math inline">\(Y\)</span>). ICC <span class="math inline">\(\approx 0.11\)</span></li>
<li><strong>Variables:</strong> LMX (Leader-Member Exchange), Gender (MALE), Team Cohesion, Team Leadership Climate</li>
</ul>
</section>
<section id="cross-level-interaction-concept" class="slide level2">
<h2>Cross-Level Interaction: Concept</h2>
<ul>
<li><strong>Definition:</strong> The influence (slope) of a Level-1 predictor (<span class="math inline">\(X_1\)</span>) on <span class="math inline">\(Y\)</span> is moderated by a Level-2 predictor (<span class="math inline">\(X_2\)</span>)</li>
<li>Often involves random slopes for <span class="math inline">\(X_1\)</span>, as the interaction term helps explain <em>why</em> slopes vary across groups</li>
<li><strong>Example:</strong>
<ul>
<li>L1 Predictor: LMX (within-team relationship quality)</li>
<li>L2 Moderator: CLIMATE (team leadership climate)</li>
<li>Interaction: Does the LMX-Empowerment relationship depend on team Climate?</li>
</ul></li>
</ul>
</section>
<section id="model-specification" class="slide level2">
<h2>Model Specification</h2>
<p><span class="math display">\[
Y_{ij} = \beta_{0j} + \beta_{1j}(X_{1ij} - \mu_{1j}) + \beta_{2}(X_{2ij}^* - \mu_{2j}) + \beta_{3}(X_{3j} - \mu_{3})
\]</span> <span class="math display">\[
+ \beta_{4}(X_{4j} - \mu_{4}) + \boldsymbol{\beta_{5}(X_{1ij} - \mu_{1j})(X_{4j} - \mu_{4})} + \epsilon_{ij}
\]</span></p>
<ul>
<li><span class="math inline">\(Y_{ij}\)</span>: Empowerment</li>
<li><span class="math inline">\(X_{1ij}\)</span>: LMX (L1 predictor, random slope <span class="math inline">\(\beta_{1j}\)</span>, latent group-mean centered)</li>
<li><span class="math inline">\(X_{2ij}^*\)</span>: MALE (L1 covariate, latent group-mean centered)</li>
<li><span class="math inline">\(X_{3j}\)</span>: COHESION (L2 covariate, grand-mean centered)</li>
<li><span class="math inline">\(X_{4j}\)</span>: CLIMATE (L2 moderator, grand-mean centered)</li>
<li><span class="math inline">\(\beta_5\)</span>: <strong>Cross-level interaction coefficient</strong></li>
</ul>
</section>
<section id="factored-regression-specification-2" class="slide level2">
<h2>Factored Regression Specification</h2>
<ul>
<li><strong>Partially Factored:</strong> Preferred due to latent mean centering <span class="math display">\[ f(Y | Xs, Interaction) \times f(X_1, X_2^*, X_3, X_4) \]</span></li>
<li><strong>Predictor Model <span class="math inline">\(f(...)\)</span>:</strong>
<ul>
<li>L1 vars (<span class="math inline">\(X_1, X_2^*\)</span>) decomposed into within/between components</li>
<li><strong>Within-Cluster:</strong> Models <span class="math inline">\((X_{1ij}, X_{2ij}^*)\)</span> around latent group means <span class="math inline">\((\mu_{1j}, \mu_{2j})\)</span> using <span class="math inline">\(\Sigma_{(W)}\)</span>. <span class="math inline">\(X_2^*\)</span> is latent response for MALE</li>
<li><strong>Between-Cluster:</strong> Models <span class="math inline">\((\mu_{1j}, \mu_{2j}, X_{3j}, X_{4j})\)</span> around grand means using <span class="math inline">\(\Sigma_{(B)}\)</span></li>
</ul></li>
</ul>
</section>
<section id="distribution-of-missing-values" class="slide level2">
<h2>Distribution of Missing Values</h2>
<ul>
<li><strong>Similar to Random Coefficient Model:</strong>
<ul>
<li>Missing <span class="math inline">\(Y\)</span>: Drawn from focal model, involves <span class="math inline">\(\beta_{0j}, \beta_{1j}\)</span></li>
<li>Missing Predictors (e.g., <span class="math inline">\(X_1\)</span> = LMX):
<ul>
<li>Conditional distribution depends on focal and predictor models</li>
<li><strong>Heteroscedasticity:</strong> Variance depends on random slope (<span class="math inline">\(\beta_{1j}^2\)</span>) and interaction term involving <span class="math inline">\(\beta_5\)</span></li>
<li>Requires methods that handle this (Bayesian/Model-Based MI). Metropolis-Hastings useful</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="three-level-models" class="slide level2">
<h2>Three-Level Models</h2>
</section>
<section id="example-educational-study-revisited-3-levels" class="slide level2">
<h2>Example: Educational Study Revisited (3 Levels)</h2>
<ul>
<li><strong>Data:</strong> Cluster-randomized trial from Sec 8.2, but now using longitudinal data</li>
<li><strong>Structure:</strong> 3-Level Hierarchy
<ul>
<li>Level 1: Measurement Occasions (<span class="math inline">\(t=1...7\)</span>, monthly)</li>
<li>Level 2: Students (<span class="math inline">\(i\)</span>)</li>
<li>Level 3: Schools (<span class="math inline">\(j\)</span>)</li>
</ul></li>
<li><strong>Outcome:</strong> Problem Solving score at each occasion (<span class="math inline">\(PROBSOLVE_{tij}\)</span>)</li>
<li><strong>Missing Data:</strong> Increased over time (~20% by final wave). Planned missingness in control group</li>
</ul>
</section>
<section id="longitudinal-growth-curve-model" class="slide level2">
<h2>Longitudinal Growth Curve Model</h2>
<ul>
<li><strong>Concept:</strong> Type of MLM where repeated measures are modeled as a function of time</li>
<li><strong>Time Predictor (<code>MONTH</code>):</strong> Codes passage of time (L1 predictor)
<ul>
<li>Example coding: Relative to final assessment (<span class="math inline">\(MONTH = -6, -5, ..., 0\)</span>). Intercept is end-of-year score</li>
</ul></li>
<li><strong>Time Variation:</strong> <code>MONTH</code> only varies within-student (L1). Constant across students and schools (No L2 or L3 variance)</li>
</ul>
</section>
<section id="level-1-model-within-student" class="slide level2">
<h2>Level-1 Model (Within-Student)</h2>
<p>Models individual change trajectories.</p>
<p><span class="math display">\[
PROBSOLVE_{tij} = \beta_{0ij} + \beta_{1ij}(MONTH_{tij}) + \epsilon_{tij}
\]</span></p>
<ul>
<li><span class="math inline">\(\beta_{0ij}\)</span>: Student <span class="math inline">\(i\)</span>’s expected score at MONTH=0 (end-of-year)</li>
<li><span class="math inline">\(\beta_{1ij}\)</span>: Student <span class="math inline">\(i\)</span>’s linear rate of change per month</li>
<li>Both <span class="math inline">\(\beta_{0ij}\)</span> and <span class="math inline">\(\beta_{1ij}\)</span> are random effects varying across students (L2) and schools (L3)</li>
<li><span class="math inline">\(\epsilon_{tij}\)</span>: Time-specific residual (<span class="math inline">\(\sim N(0, \sigma_{\epsilon}^2)\)</span>)</li>
</ul>
</section>
<section id="level-2-model-between-student" class="slide level2">
<h2>Level-2 Model (Between-Student)</h2>
<p>Models student-level variation in intercepts (<span class="math inline">\(\beta_{0ij}\)</span>) and slopes (<span class="math inline">\(\beta_{1ij}\)</span>)</p>
<p><span class="math display">\[
\beta_{0ij} = \beta_{0j} + \beta_{2}(STANMATH_{ij} - \mu_{2}) + \beta_{3}(FRLUNCH_{ij}^* - \mu_{3}) + b_{0ij}
\]</span> <span class="math display">\[
\beta_{1ij} = \beta_{1j} + b_{1ij}
\]</span></p>
<ul>
<li><span class="math inline">\(\beta_{0j}, \beta_{1j}\)</span>: School <span class="math inline">\(j\)</span>’s average intercept &amp; slope</li>
<li><span class="math inline">\(STANMATH_{ij}, FRLUNCH_{ij}^*\)</span>: Student-level (L2) covariates predicting intercept (grand-mean centered)</li>
<li><span class="math inline">\(b_{0ij}, b_{1ij}\)</span>: Student-level random effects (deviations from school means). <span class="math inline">\(\sim N_2(\mathbf{0}, \Sigma_{b(L2)})\)</span></li>
</ul>
</section>
<section id="level-3-model-between-school" class="slide level2">
<h2>Level-3 Model (Between-School)</h2>
<p>Models school-level variation in intercepts (<span class="math inline">\(\beta_{0j}\)</span>) and slopes (<span class="math inline">\(\beta_{1j}\)</span>)</p>
<p><span class="math display">\[
\beta_{0j} = \beta_{0} + \beta_{4}(TEACHEXP_{j} - \mu_{4}) + \beta_{5}(CONDITION_{j}) + b_{0j}
\]</span> <span class="math display">\[
\beta_{1j} = \beta_{1} + \beta_{6}(CONDITION_{j}) + b_{1j}
\]</span></p>
<ul>
<li><span class="math inline">\(\beta_0, \beta_1\)</span>: Grand mean intercept &amp; slope (for control group, CONDITION=0)</li>
<li><span class="math inline">\(TEACHEXP_j, CONDITION_j\)</span>: School-level (L3) covariates</li>
<li><span class="math inline">\(\beta_5\)</span>: Intercept difference for intervention group (at MONTH=0)</li>
<li><span class="math inline">\(\beta_6\)</span>: Slope difference for intervention group (Treatment <span class="math inline">\(\times\)</span> Time interaction)</li>
<li><span class="math inline">\(b_{0j}, b_{1j}\)</span>: School-level random effects. <span class="math inline">\(\sim N_2(\mathbf{0}, \Sigma_{b(L3)})\)</span></li>
</ul>
</section>
<section id="combined-model-reduced-form-1" class="slide level2">
<h2>Combined Model (Reduced Form)</h2>
<p><span class="math display">\[
Y_{tij} = (\beta_{0} + b_{0ij} + b_{0j}) + (\beta_{1} + b_{1ij} + b_{1j})MONTH_{tij} + \beta_{2}(X_{2ij} - \mu_{2})
\]</span> <span class="math display">\[
+ \beta_{3}(X_{3ij}^* - \mu_{3}) + \beta_{4}(X_{4j} - \mu_{4}) + \beta_{5}X_{5j} + \boldsymbol{\beta_{6}MONTH_{tij}X_{5j}} + \epsilon_{tij}
\]</span></p>
<ul>
<li><span class="math inline">\(Y_{tij} \sim N(E(Y_{tij}|...), \sigma_{\epsilon}^2)\)</span>. Defines distribution for missing <span class="math inline">\(Y\)</span></li>
<li>Key effects: <span class="math inline">\(\beta_5\)</span> (main effect of Condition at end-of-year), <span class="math inline">\(\beta_6\)</span> (Condition <span class="math inline">\(\times\)</span> Month interaction)</li>
</ul>
</section>
<section id="factored-regression-specification-3" class="slide level2">
<h2>Factored Regression Specification</h2>
<ul>
<li><strong>Partially Factored:</strong> <span class="math inline">\(f(Y | Xs) \times f(Xs)\)</span></li>
<li><strong>Decomposition:</strong> L1 predictors decomposed into L1(within-L2), L2(within-L3), and L3 components <span class="math display">\[ X_{1tij} = \mu_1 + (\mu_{1j}-\mu_1) + (\mu_{1ij}-\mu_{1j}) + (X_{1tij}-\mu_{1ij}) \]</span>
<ul>
<li>(Note: This formula is conceptual; specific models depend on variance components)</li>
</ul></li>
</ul>
</section>
<section id="factored-regression-predictor-models" class="slide level2">
<h2>Factored Regression: Predictor Models</h2>
<ul>
<li><strong>MONTH Predictor:</strong> Only L1 variation. Modeled as deviation from grand mean. No L2/L3 random effects needed if complete <span class="math display">\[ MONTH_{tij} = \mu_{1} + r_{1tij(W)} \]</span></li>
<li><strong>Level-2 Predictor Model:</strong> Models L2 vars (STANMATH, FRLUNCH*) around L3 latent means (<span class="math inline">\(\mu_{2j}, \mu_{3j}\)</span>) using <span class="math inline">\(\Sigma_{(L2)}\)</span> <span class="math display">\[ X_{ij(L2)} \sim N_2(\boldsymbol{\mu}_j, \Sigma_{(L2)}) \]</span></li>
<li><strong>Level-3 Predictor Model:</strong> Models L3 latent means (<span class="math inline">\(\mu_{2j}, \mu_{3j}\)</span>) + L3 vars (TEACHEXP, CONDITION*) around grand means using <span class="math inline">\(\Sigma_{(L3)}\)</span> <span class="math display">\[ X_{j(L3)} \sim N_4(\boldsymbol{\mu}, \Sigma_{(L3)}) \]</span></li>
</ul>
</section>
<section id="multiple-imputation-strategies" class="slide level2">
<h2>Multiple Imputation Strategies</h2>
</section>
<section id="recap-agnostic-vs.-model-based-mi" class="slide level2">
<h2>Recap: Agnostic vs.&nbsp;Model-Based MI</h2>
<ul>
<li><strong>Agnostic Imputation:</strong> Imputation model differs from analysis model (e.g., Joint Modeling, FCS)
<ul>
<li>Multilevel extensions exist</li>
<li>Generally suitable for <strong>random intercept</strong> models</li>
</ul></li>
<li><strong>Model-Based Imputation:</strong> Imputation model is tailored to (or is the same as) the analysis model (e.g., from Bayesian estimation)
<ul>
<li>Essential for models with <strong>random coefficients, interactions, or other nonlinearities</strong> to avoid bias</li>
</ul></li>
</ul>
</section>
<section id="caution-single-level-mi-on-multilevel-data" class="slide level2">
<h2>Caution: Single-Level MI on Multilevel Data</h2>
<ul>
<li>Applying standard (single-level) JM or FCS to multilevel data is <strong>problematic</strong></li>
<li><strong>Why?</strong> These methods ignore the data hierarchy (nesting)
<ul>
<li>They produce imputed values with <strong>no between-cluster variation</strong></li>
<li>Leads to <strong>biased estimates</strong> (e.g., attenuated L2 effects, incorrect SEs)</li>
</ul></li>
<li>Use multilevel versions of JM/FCS or model-based approaches instead</li>
</ul>
</section>
<section id="fixed-effect-imputation-concept" class="slide level2">
<h2>Fixed Effect Imputation: Concept</h2>
<ul>
<li>An alternative strategy in some situations</li>
<li><strong>Method:</strong>
<ol type="1">
<li>Create dummy variables (<span class="math inline">\(D_k\)</span>) for each Level-2 group (<span class="math inline">\(k=1...J\)</span>)</li>
<li>Include these dummy variables as predictors in a <strong>single-level</strong> imputation model</li>
<li>Effectively treats group membership as a fixed effect during imputation</li>
</ol></li>
</ul>
</section>
<section id="fixed-effect-imputation-when-to-consider" class="slide level2">
<h2>Fixed Effect Imputation: When to Consider?</h2>
<ul>
<li>May be useful when:
<ul>
<li>The number of clusters (<span class="math inline">\(J\)</span>) is very small (makes MLM estimation hard)</li>
<li>Level-2 groups are not considered a random sample from a larger population</li>
<li>Between-cluster differences are viewed as nuisance variation to be controlled, not phenomena of interest</li>
</ul></li>
</ul>
</section>
<section id="fixed-effect-imputation-model" class="slide level2">
<h2>Fixed Effect Imputation: Model</h2>
<p>Example for imputing outcome <span class="math inline">\(Y\)</span> in a random intercept context:</p>
<p><span class="math display">\[
Y_{ij} = \sum_{k=1}^{J} \gamma_{k} D_{kj} + \gamma_{J+1} X_{1ij} + \epsilon_{ij}
\]</span></p>
<ul>
<li><span class="math inline">\(D_{kj}=1\)</span> if unit <span class="math inline">\(i\)</span> is in group <span class="math inline">\(k\)</span>, 0 otherwise</li>
<li><span class="math inline">\(\gamma_k\)</span> is the estimated intercept for group <span class="math inline">\(k\)</span></li>
<li>Uses absolute coding (all <span class="math inline">\(J\)</span> dummies, no overall intercept <span class="math inline">\(\beta_0\)</span>)</li>
<li><strong>Note:</strong> Excludes L2 predictors (e.g., <span class="math inline">\(X_{2j}\)</span>) because the dummy codes account for all between-group variance in <span class="math inline">\(Y\)</span></li>
</ul>
</section>
<section id="fixed-effect-imputation-limitations" class="slide level2">
<h2>Fixed Effect Imputation: Limitations</h2>
<ul>
<li>Computationally simple</li>
<li><strong>Potential Biases:</strong>
<ul>
<li>Can overcompensate for group differences, <strong>exaggerating between-group variation</strong> (Lüdtke et al., 2017). Bias worse with low ICC or small <span class="math inline">\(n_j\)</span>.</li>
<li>May produce <strong>positively biased standard errors</strong> and inaccurate confidence intervals (Andridge, 2011; van Buuren, 2011)</li>
</ul></li>
<li><strong>Practical Limitation:</strong> Difficult to extend beyond random intercept models. Preserving random slopes would require many product terms (<span class="math inline">\(D_{kj} \times X_{1ij}\)</span>)</li>
</ul>
</section>
<section id="joint-model-jm-imputation" class="slide level2">
<h2>Joint Model (JM) Imputation</h2>
</section>
<section id="jm-imputation-overview" class="slide level2">
<h2>JM Imputation: Overview</h2>
<ul>
<li><strong>Framework:</strong> Uses a multivariate normal distribution for continuous &amp; latent response variables, decomposed into within- &amp; between-cluster parts</li>
<li><strong>Approach:</strong> Often uses an “empty” multivariate model (all variables as outcomes) for imputation</li>
<li><strong>Handles:</strong> Missing data at L1/L2, categorical variables (via latent response)</li>
<li><strong>Standard JM Limitation:</strong> Assumes common within-cluster covariance (<span class="math inline">\(\Sigma_{(W)}\)</span>) across groups. Suitable for <strong>random intercept</strong> models, but biased for random slope models</li>
</ul>
</section>
<section id="jm-model-within-cluster" class="slide level2">
<h2>JM Model: Within-Cluster</h2>
<p>Models L1 scores as correlated deviations around latent group means <span class="math inline">\(\boldsymbol{\mu}_j\)</span></p>
<p><span class="math display">\[
\mathbf{Y}_{ij(W)} = \begin{pmatrix} PROBSOLVE_{ij} \\ PRETEST_{ij} \\ STANMATH_{ij} \\ FRLUNCH_{ij}^* \end{pmatrix} = \boldsymbol{\mu}_j + \mathbf{r}_{ij(W)}
\]</span></p>
<ul>
<li>Assumes <span class="math inline">\(\mathbf{Y}_{ij(W)} \sim N_4(\boldsymbol{\mu}_j, \boldsymbol{\Sigma_{(W)}})\)</span></li>
<li><span class="math inline">\(\boldsymbol{\Sigma_{(W)}}\)</span> is the <strong>common</strong> within-cluster covariance matrix</li>
<li><span class="math inline">\(FRLUNCH^*\)</span> is latent response variable (variance fixed to 1)</li>
<li>Defines posterior predictive distribution for L1 missing values</li>
</ul>
</section>
<section id="jm-model-between-cluster" class="slide level2">
<h2>JM Model: Between-Cluster</h2>
<p>Models latent group means (<span class="math inline">\(\boldsymbol{\mu}_j\)</span>) and L2 variables</p>
<p><span class="math display">\[
\mathbf{Y}_{j(B)} = \begin{pmatrix} \mu_{1j} \\ \mu_{2j} \\ \mu_{3j} \\ \mu_{4j} \\ TEACHEXP_j \\ CONDITION_j^* \end{pmatrix} = \boldsymbol{\mu} + \mathbf{r}_{j(B)}
\]</span></p>
<ul>
<li>Assumes <span class="math inline">\(\mathbf{Y}_{j(B)} \sim N_6(\boldsymbol{\mu}, \boldsymbol{\Sigma_{(B)}})\)</span></li>
<li><span class="math inline">\(\boldsymbol{\Sigma_{(B)}}\)</span> is the between-cluster covariance matrix</li>
<li><span class="math inline">\(CONDITION^*\)</span> is latent response variable (variance fixed to 1)</li>
<li>Defines posterior predictive distribution for L2 missing values &amp; latent means</li>
</ul>
</section>
<section id="jm-mcmc-algorithm" class="slide level2">
<h2>JM MCMC Algorithm</h2>
<p>Generates imputations using Gibbs sampling:</p>
<ol type="1">
<li><strong>Initialize:</strong> Parameters (<span class="math inline">\(\boldsymbol{\mu}, \Sigma_{(W)}, \Sigma_{(B)}\)</span>), latent means (<span class="math inline">\(\boldsymbol{\mu}_j\)</span>), missing values</li>
<li><strong>Iterate (t=1…T):</strong>
<ul>
<li>Estimate grand means <span class="math inline">\(\boldsymbol{\mu}\)</span></li>
<li>Estimate latent group means <span class="math inline">\(\boldsymbol{\mu}_j\)</span></li>
<li>Estimate between-cluster covariance <span class="math inline">\(\boldsymbol{\Sigma_{(B)}}\)</span></li>
<li>Estimate within-cluster covariance <span class="math inline">\(\boldsymbol{\Sigma_{(W)}}\)</span></li>
<li>Impute missing values (using conditional MVN distributions derived from current parameters)</li>
</ul></li>
<li><strong>Repeat</strong> for M parallel chains or burn-in/thinning</li>
</ol>
</section>
<section id="jm-extension-random-within-cluster-covariances" class="slide level2">
<h2>JM Extension: Random Within-Cluster Covariances</h2>
<ul>
<li><strong>Motivation:</strong> Standard JM assumes common <span class="math inline">\(\Sigma_{(W)}\)</span>, problematic for random slopes</li>
<li><strong>Solution (Yucel, 2011):</strong> Allow <span class="math inline">\(\Sigma_{(W)}\)</span> to vary across L2 units (<span class="math inline">\(j\)</span>) <span class="math display">\[ \mathbf{Y}_{ij(W)} \sim N_k(\boldsymbol{\mu}_j, \boldsymbol{\Sigma_{j(W)}}) \]</span></li>
<li><strong>Between-Cluster Model:</strong> Remains the same</li>
<li><strong>Modeling <span class="math inline">\(\Sigma_{j(W)}\)</span>:</strong> Treat each <span class="math inline">\(\Sigma_{j(W)}\)</span> as drawn from a common <em>Wishart</em> distribution (multivariate generalization of <span class="math inline">\(\chi^2\)</span>).
<ul>
<li>Wishart defined by pooled degrees of freedom &amp; scale matrix</li>
</ul></li>
</ul>
</section>
<section id="random-covariance-mcmc" class="slide level2">
<h2>Random Covariance MCMC</h2>
<ul>
<li>Similar to standard JM MCMC, but adds steps within the loop:
<ul>
<li>Estimate pooled scale matrix (average <span class="math inline">\(\Sigma_{j(W)}\)</span> structure)</li>
<li>Estimate pooled degrees of freedom (related to average <span class="math inline">\(n_j\)</span>)</li>
<li>Estimate cluster-specific <span class="math inline">\(\boldsymbol{\Sigma_{j(W)}}\)</span> based on its data and borrowing strength via the Wishart prior</li>
</ul></li>
<li>Allows imputation even if variables are fully missing within some clusters</li>
</ul>
</section>
<section id="fully-conditional-specification-fcs-mice-imputation" class="slide level2">
<h2>Fully Conditional Specification (FCS / MICE) Imputation</h2>
</section>
<section id="fcs-imputation-overview" class="slide level2">
<h2>FCS Imputation: Overview</h2>
<ul>
<li><strong>Concept:</strong> Extends single-level FCS (MICE) to multilevel data (van Buuren, 2011)</li>
<li><strong>Method:</strong> Imputes variables one at a time using univariate regression models, conditional on all other variables. Cycles through variables iteratively</li>
<li><strong>Handles:</strong> L1, L2, L3 variables. Different model types (linear, logistic, etc.) per variable</li>
<li><strong>Standard FCS Limitation:</strong> Like JM, generally limited to <strong>random intercept</strong> models due to how random slopes are handled (or not handled)</li>
</ul>
</section>
<section id="standard-fcs-l1-imputation-models" class="slide level2">
<h2>Standard FCS: L1 Imputation Models</h2>
<p>Uses random intercept regressions for L1 variables. Example :</p>
<ul>
<li><strong>Continuous <span class="math inline">\(Y_{ij}\)</span> (e.g., PROBSOLVE):</strong> <span class="math display">\[ Y_{ij}^{(t)} = \gamma_{01j} + \gamma_{11}X_{1ij}^{(t-1)} + ... + r_{1ij} \]</span></li>
<li><strong>Binary <span class="math inline">\(Y_{ij}\)</span> (e.g., FRLUNCH):</strong> <span class="math display">\[ ln\left(\frac{Pr(Y_{ij}^{(t)}=1)}{1-Pr(Y_{ij}^{(t)}=1)}\right) = \gamma_{03j} + \gamma_{13}X_{1ij}^{(t)} + ... \]</span></li>
<li>Impute <span class="math inline">\(Y_{ij(mis)}^{(t)}\)</span> by drawing from posterior predictive distribution (Normal or Binomial). <span class="math inline">\((t)\)</span> indicates iteration</li>
</ul>
</section>
<section id="standard-fcs-l2-imputation-model" class="slide level2">
<h2>Standard FCS: L2 Imputation Model</h2>
<p>Uses single-level regression with <strong>cluster means</strong> (<span class="math inline">\(\bar{X}\)</span>) of L1 vars as predictors</p>
<p><span class="math display">\[
X_{2j}^{(t)} = \gamma_{04} + \gamma_{14}\bar{Y}_{1j}^{(t)} + \gamma_{24}\bar{X}_{1j}^{(t)} + ... + \gamma_{54}X_{L2,j} + r_{04j}
\]</span></p>
<ul>
<li><span class="math inline">\(\bar{Y}_{1j}^{(t)}, \bar{X}_{1j}^{(t)}\)</span> are arithmetic averages of (imputed) L1 variables within cluster <span class="math inline">\(j\)</span> at iteration <span class="math inline">\(t\)</span></li>
<li>Impute <span class="math inline">\(X_{2j(mis)}^{(t)}\)</span> by drawing from its posterior predictive distribution</li>
</ul>
</section>
<section id="standard-fcs-limitations" class="slide level2">
<h2>Standard FCS: Limitations</h2>
<ol type="1">
<li><strong>Common Slope Assumption:</strong> Standard specification doesn’t allow within- vs.&nbsp;between-cluster associations to differ (unlike JM)
<ul>
<li><em>Workaround:</em> Add cluster means as predictors in L1 models (mimics JM)</li>
</ul></li>
<li><strong>Assumes Equal Cluster Sizes (<span class="math inline">\(n_j\)</span>):</strong> Using arithmetic means (<span class="math inline">\(\bar{X}\)</span>) in L2 models ignores differential reliability due to varying <span class="math inline">\(n_j\)</span>. Biases tend to be small unless ICC or <span class="math inline">\(n_j\)</span> very small (Grund et al., 2017)</li>
</ol>
</section>
<section id="fcs-with-latent-variables-overview" class="slide level2">
<h2>FCS with Latent Variables: Overview</h2>
<ul>
<li><strong>Alternative Formulation:</strong> Addresses limitations of standard FCS (Enders et al., 2018; Keller &amp; Enders, 2021)</li>
<li><strong>Advantages:</strong>
<ul>
<li>Equivalent to Joint Model (JM) specification</li>
<li>Naturally handles unequal cluster sizes (<span class="math inline">\(n_j\)</span>)</li>
<li>Uses latent response variables for categorical data</li>
<li>Uses latent group means (<span class="math inline">\(\mu_j\)</span>) instead of arithmetic means (<span class="math inline">\(\bar{X}\)</span>)</li>
</ul></li>
</ul>
</section>
<section id="fcs-latent-within-cluster-regressions" class="slide level2">
<h2>FCS Latent: Within-Cluster Regressions</h2>
<p>Reparameterizes within-cluster MVN distribution (<span class="math inline">\(\Sigma_{(W)}\)</span>) as round-robin regressions</p>
<p><span class="math display">\[
Y_{1ij}^{(t)} = \mu_{1j} + \gamma_{11(W)}(Y_{2ij}^{(t-1)}-\mu_{2j}) + ... + r_{1ij(W)}
\]</span> <span class="math display">\[
Y_{2ij}^{(t)} = \mu_{2j} + \gamma_{12(W)}(Y_{3ij}^{(t-1)}-\mu_{3j}) + ... + r_{2ij(W)}
\]</span> … (one equation per L1 variable)</p>
<ul>
<li>Regressors are centered at <strong>latent group means</strong> (<span class="math inline">\(\mu_j\)</span>)</li>
<li>Intercept is the latent group mean of the variable being imputed</li>
<li>Uses latent response variables (<span class="math inline">\(Y^*\)</span>) for binary/ordinal data</li>
</ul>
</section>
<section id="fcs-latent-between-cluster-regressions" class="slide level2">
<h2>FCS Latent: Between-Cluster Regressions</h2>
<p>Reparameterizes between-cluster MVN distribution (<span class="math inline">\(\Sigma_{(B)}\)</span>) as round-robin regressions</p>
<p><span class="math display">\[
\mu_{1j}^{(t)} = \mu_{1} + \gamma_{11(B)}(\mu_{2j}^{(t-1)}-\mu_{2}) + ... + \gamma_{51(B)}(Y_{6j}^{*(t-1)}-\mu_{6}) + r_{1j(B)}
\]</span> … (one equation per latent mean <span class="math inline">\(\mu_j\)</span> and L2 variable <span class="math inline">\(Y_k\)</span>)</p>
<ul>
<li>Models latent means (<span class="math inline">\(\mu_j\)</span>) and L2 variables (<span class="math inline">\(Y_k\)</span>, possibly latent <span class="math inline">\(Y_k^*\)</span>)</li>
<li>Regressors are centered at grand means (<span class="math inline">\(\mu\)</span>)</li>
</ul>
</section>
<section id="fcs-latent-imputing-latent-means-mu_j" class="slide level2">
<h2>FCS Latent: Imputing Latent Means (<span class="math inline">\(\mu_j\)</span>)</h2>
<ul>
<li>Latent means (<span class="math inline">\(\mu_j\)</span>) are treated like missing data and updated each iteration</li>
<li>Drawn from complex conditional posterior distribution: <span class="math display">\[ f(\mu_{1j}|...) \propto \left[ \prod_{i=1}^{n_j} N(E(Y_{1ij}|...), \sigma_{Y_{1(W)}}^2) \right] \times N(E(\mu_{1j}|...), \sigma_{r_{1(B)}}^2) \]</span></li>
<li>Distribution depends on L1 data within cluster <span class="math inline">\(j\)</span> (weighted by <span class="math inline">\(n_j\)</span>) AND L2 model</li>
<li>Explicitly accounts for unequal cluster sizes <span class="math inline">\(n_j\)</span></li>
</ul>
</section>
<section id="fcs-limitation-random-coefficients" class="slide level2">
<h2>FCS Limitation: Random Coefficients</h2>
<ul>
<li>Standard FCS approaches are generally <strong>not suitable</strong> for models with random coefficients (slopes)</li>
<li><strong>Why?</strong> The way FCS imputes the L1 predictor involved in the random slope is often incompatible with the analysis model</li>
</ul>
</section>
<section id="problem-reverse-random-coefficient-imputation" class="slide level2">
<h2>Problem: Reverse Random Coefficient Imputation</h2>
<ul>
<li>Consider analysis model: <span class="math inline">\(Y_{ij} = \beta_{0j} + \beta_{1j}X_{ij} + \epsilon_{ij}\)</span></li>
<li>A seemingly plausible FCS imputation model for <span class="math inline">\(X_{ij}\)</span> might be: <span class="math display">\[ X_{ij} = \gamma_{0j} + \gamma_{1j}Y_{ij} + r_{ij} \]</span> (i.e., a random coefficient model predicting <span class="math inline">\(X\)</span> from <span class="math inline">\(Y\)</span>)</li>
<li><strong>Incompatibility:</strong> These two models are logically inconsistent unless the random slope variance (<span class="math inline">\(\sigma^2_{\beta_1}\)</span>) is zero. They cannot arise from the same underlying joint distribution</li>
</ul>
</section>
<section id="issue-heteroscedasticity-ignored" class="slide level2">
<h2>Issue: Heteroscedasticity Ignored</h2>
<ul>
<li>Recall the <em>correct</em> conditional distribution for <span class="math inline">\(X_{1ij(mis)}\)</span> from Bayesian perspective: <span class="math display">\[ Var(X_{1ij}|Y_{ij}, ...) = \left( \frac{1}{\sigma_{r_{1(W)}}^2} + \frac{\boldsymbol{\beta_{1j}^2}}{\sigma_{\epsilon}^2} \right)^{-1} \]</span></li>
<li>The variance depends on the cluster-specific <span class="math inline">\(\beta_{1j}^2\)</span> (heteroscedastic)</li>
<li>The reverse regression model incorrectly assumes a <em>constant</em> residual variance (<span class="math inline">\(\sigma_r^2\)</span>) for <span class="math inline">\(X_{ij}\)</span> across all clusters <span class="math inline">\(j\)</span>. It fails to capture the necessary heteroscedasticity</li>
</ul>
</section>
<section id="consequences-recommendation" class="slide level2">
<h2>Consequences &amp; Recommendation</h2>
<ul>
<li><strong>Bias:</strong> Using standard FCS (incl.&nbsp;reverse random coefficient models) when the analysis model has random slopes leads to bias, particularly <strong>underestimation of the random slope variance</strong>.</li>
<li><strong>Connection:</strong> Similar issue to “just-another-variable” imputation for interaction models (Sec 5.4). Random slope models are a type of interaction (<span class="math inline">\(X_{1ij} \times \beta_{1j}\)</span>)</li>
<li><strong>Recommendation:</strong> For random coefficient models, use <strong>Bayesian estimation</strong> or <strong>Model-Based MI</strong> derived from the correctly specified factored regression model. Avoid standard FCS</li>
</ul>
</section>
<section id="maximum-likelihood-ml-estimation" class="slide level2">
<h2>Maximum Likelihood (ML) Estimation</h2>
</section>
<section id="ml-overview" class="slide level2">
<h2>ML Overview</h2>
<ul>
<li>Currently arguably <strong>less capable</strong> than Bayesian/MI for complex multilevel missing data problems (e.g., random slopes with missing predictors)</li>
<li>Handles a more limited set of scenarios effectively</li>
</ul>
</section>
<section id="ml-incomplete-outcomes-only" class="slide level2">
<h2>ML: Incomplete Outcomes Only</h2>
<ul>
<li><strong>Handled by standard mixed model software</strong> (e.g., <code>lme4</code>, <code>nlme</code>, SAS PROC MIXED, SPSS MIXED)</li>
<li>If missingness depends only on observed predictors (MAR), analyzing the observed <span class="math inline">\(Y\)</span> values gives valid ML estimates</li>
<li>No imputation needed; equivalent to complete-data estimation with unbalanced cluster sizes (<span class="math inline">\(n_j\)</span>)</li>
</ul>
</section>
<section id="ml-incomplete-predictors---challenges" class="slide level2">
<h2>ML: Incomplete Predictors - Challenges</h2>
<ul>
<li>Many standard MLM packages <strong>default to listwise deletion</strong> if predictors are missing
<ul>
<li>Assumes MCAR (often unrealistic)</li>
<li>Reduces sample size, potentially discarding entire clusters</li>
</ul></li>
<li>Need specialized ML approaches</li>
</ul>
</section>
<section id="ml-approach-1-hlm-software" class="slide level2">
<h2>ML Approach 1: HLM Software</h2>
<ul>
<li><strong>Method:</strong> Shin &amp; Raudenbush (2007, 2013) approach implemented in HLM software</li>
<li><strong>Assumptions:</strong> Incomplete predictors are multivariate normal (MVN)</li>
<li><strong>Limitations:</strong>
<ul>
<li>No capacity for incomplete categorical predictors</li>
<li>No capacity for random slopes between <em>incomplete</em> L1 variables</li>
</ul></li>
<li><strong>Mechanism:</strong> Reparameterizes MLM into within/between MVN components (like JM). Uses EM algorithm to estimate means/covariances, transforms back to regression parameters</li>
</ul>
</section>
<section id="ml-approach-2-multilevel-sem" class="slide level2">
<h2>ML Approach 2: Multilevel SEM</h2>
<ul>
<li><strong>Method:</strong> Use Full Information Maximum Likelihood (FIML) within a Structural Equation Modeling framework</li>
<li><strong>Assumption:</strong> Typically assumes incomplete variables are MVN</li>
<li><strong>Capability:</strong> Can <em>specify</em> models with incomplete random slope predictors</li>
<li><strong>Limitation:</strong> Prone to <strong>substantial bias</strong> when predictors in random slopes are missing (Enders et al., 2018, 2020). Fails to account for heteroscedasticity</li>
<li><strong>Recommendation:</strong> Best suited for <strong>random intercept</strong> models when predictors are missing (assuming MVN)</li>
</ul>
</section>
<section id="multilevel-sem-framework" class="slide level2">
<h2>Multilevel SEM Framework</h2>
<ul>
<li><strong>Recap SEM (Ch 3):</strong> Models individual data vector <span class="math inline">\(\mathbf{Y}_i \sim N(\boldsymbol{\mu}(\theta), \boldsymbol{\Sigma}(\theta))\)</span>. FIML uses available data for each case</li>
<li><strong>Multilevel SEM:</strong> Unit of analysis is the cluster (<span class="math inline">\(j\)</span>). <span class="math inline">\(\mathbf{Y}_j\)</span> is the vector of all L1 observations for cluster <span class="math inline">\(j\)</span>
<ul>
<li><span class="math inline">\(\mathbf{Y}_j = (Y_{1j}, Y_{2j}, ..., Y_{n_j, j})'\)</span></li>
<li>Assumes <span class="math inline">\(\mathbf{Y}_j\)</span> follows a multivariate normal distribution with a structured mean vector <span class="math inline">\(\boldsymbol{\mu}_j(\theta)\)</span> and covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}_j(\theta)\)</span> derived from the MLM parameters <span class="math inline">\(\theta\)</span></li>
</ul></li>
</ul>
</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<ul>
<li>Multilevel Data: Characterized by hierarchical nesting (e.g., measurements within persons, students within schools), leading to variance and covariance at multiple levels</li>
<li>Models Discussed: The chapter covers random intercept, random coefficient (random slope), cross-level interaction, and three-level (e.g., growth curve) models. Centering strategies (latent group-mean, grand-mean) are crucial</li>
<li>Core Missing Data Challenge: Handling missing predictors, especially Level-1 predictors involved in random slopes or interactions, is complex due to induced heteroscedasticity in their conditional distributions</li>
</ul>

</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p><a href="https://jonathantemplin.github.io/MissingDataMethods2025/">https://jonathantemplin.github.io/MissingDataMethods2025/</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="09_Multilevel_Missing_Data_And_NMAR_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="09_Multilevel_Missing_Data_And_NMAR_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="09_Multilevel_Missing_Data_And_NMAR_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="09_Multilevel_Missing_Data_And_NMAR_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="09_Multilevel_Missing_Data_And_NMAR_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="09_Multilevel_Missing_Data_And_NMAR_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="09_Multilevel_Missing_Data_And_NMAR_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="09_Multilevel_Missing_Data_And_NMAR_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="09_Multilevel_Missing_Data_And_NMAR_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="09_Multilevel_Missing_Data_And_NMAR_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>