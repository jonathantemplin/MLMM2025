<!DOCTYPE html>
<html lang="en"><head>
<script src="08_Multiple_Imputation_files/libs/clipboard/clipboard.min.js"></script>
<script src="08_Multiple_Imputation_files/libs/quarto-html/tabby.min.js"></script>
<script src="08_Multiple_Imputation_files/libs/quarto-html/popper.min.js"></script>
<script src="08_Multiple_Imputation_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="08_Multiple_Imputation_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="08_Multiple_Imputation_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="08_Multiple_Imputation_files/libs/quarto-html/quarto-syntax-highlighting-f23ab18612d661d7bd56dbc0c0fd3817.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.34">

  <meta name="author" content="Lecture 8: April 9, 2025">
  <title>Multiple Imputation</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="08_Multiple_Imputation_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="08_Multiple_Imputation_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="08_Multiple_Imputation_files/libs/revealjs/dist/theme/quarto-16232f29d62c098d89a38a65e10d436d.css">
  <link href="08_Multiple_Imputation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="08_Multiple_Imputation_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="08_Multiple_Imputation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="08_Multiple_Imputation_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Multiple Imputation</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Lecture 8: April 9, 2025 
</div>
</div>
</div>

</section>
<section id="goal-of-missing-data-analysis-methods" class="slide level2">
<h2>Goal of Missing Data Analysis Methods</h2>
<ul>
<li><strong>Maximum Likelihood (ML) &amp; Bayesian Estimation:</strong>
<ul>
<li>Primary goal: Fit a model to observed data and use estimates for research questions</li>
<li>Missing data handling occurs “behind the scenes”</li>
<li>ML deduces missing parts via the normal curve; Bayesian imputes values en route to parameters</li>
<li>Imputation is a means to an end (getting estimates)</li>
</ul></li>
<li><strong>Multiple Imputation (MI):</strong>
<ul>
<li>Puts filled-in data “front and center”</li>
<li>Goal: Create suitable imputations for <em>later</em> analysis</li>
</ul></li>
</ul>
</section>
<section id="the-multiple-imputation-process-3-steps" class="slide level2">
<h2>The Multiple Imputation Process: 3 Steps</h2>
<ol type="1">
<li><strong>Imputation:</strong>
<ul>
<li>Specify an imputation model</li>
<li>Use an MCMC algorithm to create multiple (M) copies of the data, each with different imputed values</li>
<li>Often uses MCMC algorithms for regression models or covariance matrices</li>
</ul></li>
<li><strong>Analysis:</strong>
<ul>
<li>Perform the desired analysis (or analyses) on <em>each</em> of the M completed datasets</li>
<li>Obtain point estimates and standard errors from each analysis</li>
<li>Compatible with the frequentist statistical paradigm</li>
</ul></li>
<li><strong>Pooling:</strong>
<ul>
<li>Combine the estimates and standard errors from the M analyses into a single set of results</li>
<li>Uses “Rubin’s rules”</li>
</ul></li>
</ol>
</section>
<section id="history-chapter-focus" class="slide level2">
<h2>History &amp; Chapter Focus</h2>
<ul>
<li><strong>Origins:</strong> Proposed by Donald Rubin in 1977 for missing survey data. Seminal text published in 1987. Usage has grown significantly since</li>
<li><strong>Diversity:</strong> MI includes a diverse collection of procedures</li>
<li><strong>Chapter Focus:</strong>
<ul>
<li>Strategies based on Bayesian estimation routines for regression and multivariate normal data</li>
<li>Includes Joint Model Imputation and Fully Conditional Specification (FCS)</li>
<li>Emphasis on analyzing the imputed datasets and summarizing results</li>
</ul></li>
</ul>
</section>
<section id="two-rounds-of-modeling" class="slide level2">
<h2>Two Rounds of Modeling</h2>
<p>Multiple imputation involves two distinct modeling steps:</p>
<ol type="1">
<li><strong>Imputation Model:</strong>
<ul>
<li>Fitted to the observed data</li>
<li>Uses resulting estimates (often via MCMC) to create imputed datasets</li>
</ul></li>
<li><strong>Analysis Model:</strong>
<ul>
<li>The model(s) of substantive interest</li>
<li>Fitted to <em>each</em> of the imputed datasets</li>
<li>Results are aggregated (pooled)</li>
</ul></li>
</ol>
<p><em>Important Note:</em> The imputation and analysis models do not need to be the same statistical model</p>
</section>
<section id="agnostic-imputation" class="slide level2">
<h2>Agnostic Imputation</h2>
<ul>
<li><strong>Definition:</strong> The imputation model <em>differs</em> from the substantive analysis model</li>
<li><strong>Goal:</strong> To apply a flexible, nonrestrictive model not dedicated to one specific analysis</li>
<li>The resulting imputed datasets could potentially be used for several different analyses</li>
<li><strong>Crucial:</strong> The imputation model must still be flexible enough to preserve important features of the planned secondary analyses and not impose conflicting restrictions</li>
</ul>
</section>
<section id="model-based-imputation" class="slide level2">
<h2>Model-Based Imputation</h2>
<ul>
<li><strong>Definition:</strong> The imputation model <em>is the same</em> as the focal analysis model (though it might include additional auxiliary variables)</li>
<li><strong>Goal:</strong> To tailor the imputations specifically for one analysis model</li>
<li>This approach is necessarily narrow in scope compared to agnostic imputation</li>
</ul>
</section>
<section id="when-to-use-which" class="slide level2">
<h2>When to Use Which?</h2>
<p>The choice depends on the <strong>composition of the analysis model</strong>:</p>
<ul>
<li><p><strong>Agnostic Imputation:</strong> Well-suited for analyses that <em>do not</em> include special features like interactions, polynomial terms, or random effects. Often intermediate in scope (e.g., for all analyses in one paper).</p></li>
<li><p><strong>Model-Based Imputation:</strong> Usually ideal when the analysis model <em>does</em> include nonlinear effects (interactions, polynomials, random effects).</p></li>
</ul>
<p><em>Note:</em> It’s acceptable to use both approaches within the same project if needed</p>
</section>
<section id="joint-model-imputation" class="slide level2">
<h2>Joint Model Imputation</h2>
</section>
<section id="core-idea-development" class="slide level2">
<h2>Core Idea &amp; Development</h2>
<ul>
<li><strong>Foundation:</strong> Derives from applying a multivariate distribution to a set of incomplete variables</li>
<li><strong>Popularization:</strong> Largely attributed to Joe Schafer (1997, 1999). Early methods focused on multivariate normal or multinomial distributions.</li>
<li><strong>Modern Approach:</strong> Contemporary variants often (but certainly not always) use latent response formulations to handle mixtures of continuous, binary, ordinal, and nominal variables</li>
</ul>
</section>
<section id="example-math-achievement-data" class="slide level2">
<h2>Example: Math Achievement Data</h2>
<ul>
<li><p><strong>Data:</strong> Pretest (MATHPRE) and Posttest (MATHPOST) math scores, plus academic variables (<span class="math inline">\(N=250\)</span>)</p></li>
<li><p><strong>Missingness:</strong> Pretest complete, Posttest 16.8% missing.</p></li>
<li><p><strong>Analysis Goal:</strong> Examine change over time using a difference score: <span class="math inline">\(CHANGE_i = Y_{2i} - Y_{1i} = MATHPOST_i - MATHPRE_i\)</span></p></li>
<li><p><strong>Analysis Model (Empty Regression):</strong></p>
<p><span class="math display">\[CHANGE_i = \beta_0 + \epsilon_i,\]</span></p></li>
</ul>
<p>where <span class="math inline">\(CHANGE_i \sim N_1(\beta_0, \sigma_{\epsilon}^2)\)</span> (<span class="math inline">\(\beta_0\)</span> is the average change)</p>
</section>
<section id="incorporating-auxiliary-variables" class="slide level2">
<h2>Incorporating Auxiliary Variables</h2>
<ul>
<li><strong>Benefit:</strong> MI readily accommodates an <em>inclusive</em> analysis strategy using auxiliary variables</li>
<li><strong>Included Variables:</strong>
<ul>
<li>Standardized reading scores (STANREAD) - 10.4% missing</li>
<li>Binary Free/Reduced Lunch indicator (FRLUNCH) - 5.2% missing</li>
</ul></li>
<li><strong>Purpose:</strong> Although not predicting missingness directly here, they improve power by explaining variance in posttest scores (“Type B” auxiliary variables)</li>
</ul>
</section>
<section id="joint-model-jm-imputation-model-setup" class="slide level2">
<h2>Joint Model (JM) Imputation Model Setup</h2>
<ul>
<li><strong>Variables Included:</strong>
<ul>
<li><span class="math inline">\(Y_1\)</span>: MATHPRE (Pretest Math)</li>
<li><span class="math inline">\(Y_2\)</span>: MATHPOST (Posttest Math)</li>
<li><span class="math inline">\(Y_3\)</span>: STANREAD (Standardized Reading)</li>
<li><span class="math inline">\(Y_4\)</span>: FRLUNCH (Lunch Assistance - Binary)</li>
</ul></li>
<li><strong>Latent Variable:</strong> The binary <span class="math inline">\(Y_4\)</span> is modeled via a latent continuous variable <span class="math inline">\(Y_4^*\)</span>.</li>
<li><strong>Model:</strong> An empty multivariate regression using a multivariate normal distribution for <span class="math inline">\(Y_1, Y_2, Y_3, Y_4^*\)</span>: <span class="math display">\[
  \begin{pmatrix} Y_{1i} \\ Y_{2i} \\ Y_{3i} \\ Y_{4i}^* \end{pmatrix} \sim N_4 \left( \begin{pmatrix} \mu_1 \\ \mu_2 \\ \mu_3 \\ \mu_4 \end{pmatrix}, \mathbf{\Sigma} \right)
  \]</span>
<ul>
<li><span class="math inline">\(\boldsymbol{\mu}\)</span> is the mean vector.</li>
<li><span class="math inline">\(\mathbf{\Sigma}\)</span> is the covariance matrix (variance of <span class="math inline">\(Y_4^*\)</span> fixed to identify the metric).</li>
</ul></li>
</ul>
</section>
<section id="model-visualization-type" class="slide level2">
<h2>Model Visualization &amp; Type</h2>

<img data-src="s7.3modelA.png" class="r-stretch quarto-figure-center"><p class="caption">Figure 7.1</p></section>
<section id="model-visualization-type-1" class="slide level2">
<h2>Model Visualization &amp; Type</h2>
<ul>
<li><strong>Path Diagram:</strong> Figure 7.1 visualizes this model.
<ul>
<li>Rectangles for observed variables (<span class="math inline">\(Y_1, Y_2, Y_3, Y_4\)</span>)</li>
<li>Oval for the latent variable (<span class="math inline">\(Y_4^*\)</span>)</li>
<li>Broken arrow (<span class="math inline">\(Y_4^* \to Y_4\)</span>) represents the link function (threshold)</li>
<li>Curved arrows represent covariances</li>
</ul></li>
<li><strong>Agnostic Approach:</strong> This imputation model is considered <em>agnostic</em>
<ul>
<li>It differs substantially from the simple analysis model (Equation 7.2)</li>
<li>However, the multivariate normal assumption for imputation doesn’t conflict with the analysis model’s assumptions</li>
</ul></li>
</ul>
</section>
<section id="missing-data-imputation-in-joint-models" class="slide level2">
<h2>Missing Data Imputation in Joint Models</h2>
</section>
<section id="the-logic-from-parameters-to-regressions" class="slide level2">
<h2>The Logic: From Parameters to Regressions</h2>
<ul>
<li>Imputing multivariate normal data involves converting the estimated parameters (mean vector <span class="math inline">\(\boldsymbol{\mu}\)</span> and covariance matrix <span class="math inline">\(\mathbf{\Sigma}\)</span>) into a series of regression models</li>
<li>A specific regression model is derived for each unique missing data pattern observed in the data</li>
<li>MCMC is used to sample imputations based on these derived regressions</li>
</ul>
</section>
<section id="example-missing-only-y_2-posttest-math" class="slide level2">
<h2>Example: Missing Only <span class="math inline">\(Y_2\)</span> (Posttest Math)</h2>
<ul>
<li>For cases missing only <span class="math inline">\(Y_2\)</span>, the process requires the regression of <span class="math inline">\(Y_2\)</span> on the observed variables (<span class="math inline">\(Y_1, Y_3, Y_4^*\)</span>).</li>
<li>MCMC samples imputations for <span class="math inline">\(Y_{2i(mis)}\)</span> from its conditional normal distribution: <span class="math display">\[
  Y_{2i(mis)} \sim N_1(E(Y_2|Y_1, Y_3, Y_4^*), \sigma_{2|134}^2)
  \]</span>
<ul>
<li><span class="math inline">\(E(Y_2|...)\)</span> is the predicted score (mean of the distribution)</li>
<li><span class="math inline">\(\sigma_{2|134}^2\)</span> is the residual variance (spread of the distribution)</li>
</ul></li>
</ul>
</section>
<section id="example-missing-y_2-and-y_4-posttest-frlunch" class="slide level2">
<h2>Example: Missing <span class="math inline">\(Y_2\)</span> and <span class="math inline">\(Y_4^*\)</span> (Posttest &amp; FRLunch*)</h2>
<ul>
<li>For cases missing both <span class="math inline">\(Y_2\)</span> and the latent <span class="math inline">\(Y_4^*\)</span>, imputation requires the multivariate regression of <span class="math inline">\((Y_2, Y_4^*)\)</span> on the observed variables (<span class="math inline">\(Y_1, Y_3\)</span>)</li>
<li>MCMC samples imputations from a bivariate normal distribution: <span class="math display">\[
  \begin{pmatrix} Y_{2i(mis)} \\ Y_{4i(mis)}^* \end{pmatrix} \sim N_2 \left( \begin{pmatrix} E(Y_2|Y_1, Y_3) \\ E(Y_4^*|Y_1, Y_3) \end{pmatrix}, \mathbf{\Sigma}_{24|13} \right)
  \]</span>
<ul>
<li>Imputations are predicted values plus error (correlation via off-diagonals in <span class="math inline">\(\mathbf{\Sigma}_{24|13}\)</span>)</li>
</ul></li>
</ul>
</section>
<section id="imputing-categorical-variables-via-latent" class="slide level2">
<h2>Imputing Categorical Variables (via Latent)</h2>
<ul>
<li>Notice that imputations for the binary FRLUNCH variable (<span class="math inline">\(Y_4\)</span>) are generated on the <em>latent</em> metric (<span class="math inline">\(Y_4^*\)</span>)
<ul>
<li>This isn’t fully necessary as one can also impute the binary variable directly (which the R package <code>mice</code> does)</li>
</ul></li>
<li>The discrete imputed value (0 or 1) is determined by the location of the latent imputation (<span class="math inline">\(Y_{4i(mis)}^*\)</span>) relative to the estimated threshold parameter
<ul>
<li>e.g., <span class="math inline">\(Y_{4i(mis)}^* &gt; \tau \implies Y_{4(mis)} = 1\)</span></li>
<li>e.g., <span class="math inline">\(Y_{4i(mis)}^* \le \tau \implies Y_{4(mis)} = 0\)</span></li>
</ul></li>
<li>The resulting <em>dichotomous</em> imputations are only needed when saving the final imputed dataset, not during the MCMC estimation process itself</li>
<li>This is based on a formal link, not an ad hoc rounding scheme</li>
</ul>
</section>
<section id="saving-filled-in-data-sets-for-later-analysis" class="slide level2">
<h2>Saving Filled-In Data Sets for Later Analysis</h2>
</section>
<section id="goal-basic-mcmc-process" class="slide level2">
<h2>Goal &amp; Basic MCMC Process</h2>
<ul>
<li><strong>Goal:</strong> Joint model imputation uses Bayesian estimation, but the primary aim is saving filled-in datasets, not interpreting the imputation model parameters themselves</li>
<li><strong>MCMC Steps (per iteration):</strong>
<ol type="1">
<li>Estimate mean vector (<span class="math inline">\(\boldsymbol{\mu}\)</span>) conditional on current covariance (<span class="math inline">\(\mathbf{\Sigma}\)</span>) and data</li>
<li>Estimate covariance matrix (<span class="math inline">\(\mathbf{\Sigma}\)</span>) conditional on new <span class="math inline">\(\boldsymbol{\mu}\)</span> and data</li>
<li>Update missing values (including latent scores) conditional on current <span class="math inline">\(\boldsymbol{\mu}\)</span> and <span class="math inline">\(\mathbf{\Sigma}\)</span> <em>(+ estimate thresholds for ordinal variables)</em></li>
</ol></li>
</ul>
</section>
<section id="the-autocorrelation-problem" class="slide level2">
<h2>The Autocorrelation Problem</h2>
<ul>
<li><strong>Issue:</strong> MCMC estimation yields highly correlated results from one iteration to the next</li>
<li><strong>Impact:</strong> Serial dependencies among a small number of saved imputed datasets (e.g., <span class="math inline">\(M=20\)</span>) can attenuate the final multiple imputation standard errors</li>
<li><strong>Conclusion:</strong> You can’t simply save the <span class="math inline">\(M\)</span> datasets from <span class="math inline">\(M\)</span> successive iterations right after the burn-in period</li>
</ul>
</section>
<section id="solution-1-sequential-chains-thinning" class="slide level2">
<h2>Solution 1: Sequential Chains (Thinning)</h2>
<ul>
<li><strong>Method:</strong> Run a <em>single</em>, long MCMC process (e.g., <span class="math inline">\(M \times T\)</span> iterations)</li>
<li><strong>Saving:</strong> Save datasets at pre-specified intervals of <span class="math inline">\(T\)</span> iterations (the “thinning interval”) after an initial burn-in period (often also <span class="math inline">\(T\)</span> iterations)</li>
<li><strong>Recipe:</strong>
<ol type="1">
<li>Initialize parameters &amp; missing data</li>
<li><em>Loop m = 1 to M (imputations):</em>
<ol type="a">
<li><em>Loop t = 1 to T (thinning interval):</em>
<ol type="i">
<li>Estimate parameters</li>
<li>Estimate missing values</li>
</ol></li>
<li>Save the filled-in data</li>
</ol></li>
</ol></li>
<li><strong>Goal of T:</strong> Choose a thinning interval <span class="math inline">\(T\)</span> large enough to reduce autocorrelation between saved datasets</li>
</ul>
</section>
<section id="solution-2-parallel-chains" class="slide level2">
<h2>Solution 2: Parallel Chains</h2>
<ul>
<li><strong>Method:</strong> Initiate <span class="math inline">\(M\)</span> <em>separate</em>, unique MCMC processes</li>
<li><strong>Starting Values:</strong> Use different (e.g., random) starting values for each chain</li>
<li><strong>Saving:</strong> Run each chain for <span class="math inline">\(T\)</span> iterations (burn-in period) and save the filled-in data from the <em>last iteration</em> of each chain</li>
<li><strong>Recipe:</strong>
<ol type="1">
<li><em>Loop m = 1 to M (imputations):</em>
<ol type="a">
<li>Initialize parameters &amp; missing data (new start values each time)</li>
<li><em>Loop t = 1 to T (burn-in iterations):</em>
<ol type="i">
<li>Estimate parameters</li>
<li>Estimate missing values</li>
</ol></li>
<li>Save the filled-in data</li>
</ol></li>
</ol></li>
<li><strong>Advantage:</strong> Naturally avoids autocorrelation because chains are independent</li>
</ul>
</section>
<section id="important-prerequisite-convergence-check" class="slide level2">
<h2>Important Prerequisite: Convergence Check</h2>
<ul>
<li><strong>Before Saving:</strong> Regardless of using sequential or parallel chains, it’s crucial to evaluate MCMC convergence <em>before</em> saving the first imputed dataset</li>
<li><strong>Tools:</strong> Use standard diagnostics like:
<ul>
<li>Trace plots</li>
<li>Potential Scale Reduction Factor (PSRF; <span class="math inline">\(\hat{R}\)</span>)</li>
</ul></li>
<li><strong>Goal:</strong> Ensure the MCMC process has converged and is mixing well</li>
</ul>
</section>
<section id="how-many-imputations-are-needed" class="slide level2">
<h2>How Many Imputations Are Needed?</h2>
</section>
<section id="the-core-question" class="slide level2">
<h2>The Core Question</h2>
<ul>
<li><strong>Contrast:</strong>
<ul>
<li>Bayesian analysis summarizes posterior distributions over thousands of MCMC iterations</li>
<li>Multiple Imputation (MI) performs frequentist analyses on a much smaller collection (<span class="math inline">\(M\)</span>) of filled-in datasets</li>
</ul></li>
<li><strong>Question:</strong> How many imputed datasets (<span class="math inline">\(M\)</span>) are necessary for the secondary analysis phase</li>
</ul>
</section>
<section id="early-recommendations-m-3-to-5" class="slide level2">
<h2>Early Recommendations: <span class="math inline">\(M = 3\)</span> to 5</h2>
<ul>
<li><strong>Basis:</strong> Early recommendations stemmed from statistical precision (relative efficiency)</li>
<li><strong>Rationale:</strong> The average squared error of an estimate using <span class="math inline">\(M=3\)</span> to 5 imputations is only slightly worse than the precision achievable with an infinite number of imputations</li>
</ul>
</section>
<section id="beyond-precision-statistical-power" class="slide level2">
<h2>Beyond Precision: Statistical Power</h2>
<ul>
<li><strong>Finding (Graham et al., 2007):</strong> Maximizing relative efficiency doesn’t necessarily maximize statistical power to detect effects</li>
<li><strong>Recommendation:</strong> Non-trivial power gains can often be achieved by analyzing <span class="math inline">\(M=20\)</span> to 100 datasets</li>
<li><strong>Factor:</strong> The optimal <span class="math inline">\(M\)</span> for power tends to increase as the amount of missing information increases</li>
</ul>
</section>
<section id="beyond-power-reducing-monte-carlo-error" class="slide level2">
<h2>Beyond Power: Reducing Monte Carlo Error</h2>
<ul>
<li><strong>Issue:</strong> Using too few imputations can introduce noise (Monte Carlo error) into results</li>
<li><strong>Recommendation:</strong> Other studies suggest <span class="math inline">\(M=100\)</span> or more may be necessary to:
<ul>
<li>Reduce the impact of simulation noise on standard errors</li>
<li>Get stable estimates of confidence interval widths and p-values</li>
<li>Obtain good estimates of the fraction of missing information (FMI)</li>
</ul></li>
</ul>
</section>
<section id="practical-recommendation" class="slide level2">
<h2>Practical Recommendation</h2>
<ul>
<li><strong>Moore’s Law:</strong> Increased computing power makes generating more imputations feasible</li>
<li><strong>Takeaway:</strong> There is often little practical reason <em>not</em> to use many datasets (e.g., <span class="math inline">\(M=100\)</span>)</li>
<li><strong>Benefit:</strong> Using a larger number of imputations (<span class="math inline">\(M=100\)</span> or more) can help address concerns related to power and Monte Carlo error</li>
<li>The author uses <span class="math inline">\(M=100\)</span> for most examples in the text</li>
</ul>
</section>
<section id="summary-connection-to-example" class="slide level2">
<h2>Summary &amp; Connection to Example</h2>
<ul>
<li><strong>Joint Model (JM) Imputation:</strong>
<ul>
<li>Assumes incomplete variables follow a multivariate distribution (often multivariate normal, potentially with latent variables for categorical data)</li>
<li>Imputes missing values based on the conditional distributions derived from the estimated joint distribution parameters (<span class="math inline">\(\boldsymbol{\mu}, \mathbf{\Sigma}\)</span>)</li>
<li>Requires careful consideration of MCMC convergence and autocorrelation when saving imputed datasets (using thinning or parallel chains)</li>
<li>Number of imputations (<span class="math inline">\(M\)</span>) impacts power and stability (often <span class="math inline">\(M \ge 20\)</span>, practically <span class="math inline">\(M=100+\)</span> used)</li>
</ul></li>
<li>See the R script for this lecture for the example</li>
</ul>
</section>
<section id="fully-conditional-specification-fcs" class="slide level2">
<h2>Fully Conditional Specification (FCS)</h2>
</section>
<section id="core-idea-contrast-with-joint-models-jm" class="slide level2">
<h2>Core Idea &amp; Contrast with Joint Models (JM)</h2>
<ul>
<li><strong>Alternative Name:</strong> Fully Conditional Specification (FCS) is also known as <strong>Chained Equations</strong> imputation (often associated with the MICE algorithm/package)</li>
<li><strong>Contrast with JM:</strong> Instead of working from an assumed multivariate distribution (like JM), FCS specifies the imputation model as a <strong>sequence of univariate regression models</strong>, one for each variable with missing data</li>
<li><strong>Flexibility:</strong> Naturally handles mixtures of categorical and continuous variables, skip patterns, or other situations where a single joint distribution is hard to specify</li>
</ul>
</section>
<section id="the-chained-equations-mechanism" class="slide level2">
<h2>The “Chained Equations” Mechanism</h2>
<ul>
<li><strong>Process:</strong> Imputes variables one at a time using a “round-robin” scheme (each variable is predicted by all others)</li>
<li><strong>Steps:</strong>
<ol type="1">
<li>For an incomplete variable <span class="math inline">\(Y_v\)</span>, specify a regression model predicting <span class="math inline">\(Y_v\)</span> from all other variables <span class="math inline">\(Y_{-v}\)</span> (complete variables <span class="math inline">\(X\)</span> and other incomplete variables <span class="math inline">\(Y\)</span>)</li>
<li>Cycle through each incomplete variable, updating its imputed values based on the current values of the other variables</li>
</ol></li>
<li><strong>General Model for <span class="math inline">\(Y_v\)</span> (at iteration <span class="math inline">\(t\)</span>):</strong> <span class="math display">\[ Y_{vi}^{(t)} \sim p(Y_{vi} | Y_{1i}^{(t)}, ..., Y_{(v-1)i}^{(t)}, Y_{(v+1)i}^{(t-1)}, ..., Y_{Vi}^{(t-1)}, X_i, \boldsymbol{\phi}_v) \]</span>
<ul>
<li>Imputation for <span class="math inline">\(Y_v\)</span> depends on the most recently imputed values of other variables</li>
<li><span class="math inline">\(p(...)\)</span> is the conditional distribution for <span class="math inline">\(Y_v\)</span>, with parameters <span class="math inline">\(\boldsymbol{\phi}_v\)</span></li>
</ul></li>
</ul>
</section>
<section id="role-of-mcmc-handling-mixed-types" class="slide level2">
<h2>Role of MCMC &amp; Handling Mixed Types</h2>
<ul>
<li><strong>MCMC:</strong> Provides the Bayesian framework for estimating the parameters (<span class="math inline">\(\boldsymbol{\phi}_v\)</span>) of each conditional regression model in the sequence</li>
<li><strong>Mixed Variable Types:</strong> FCS easily handles different types of variables by tailoring the regression model used for each variable:
<ul>
<li><strong>Continuous:</strong> Linear regression (impute from <span class="math inline">\(N(\text{predicted value}, \sigma^2_{residual})\)</span>).</li>
<li><strong>Binary:</strong> Logistic or Probit regression (impute 0/1 based on predicted probability)</li>
<li><strong>Ordinal:</strong> Ordinal logistic or Probit regression</li>
<li><strong>Nominal:</strong> Multinomial logistic regression</li>
<li>… and so on</li>
</ul></li>
</ul>
</section>
<section id="example-math-achievement-models-conceptual" class="slide level2">
<h2>Example: Math Achievement Models (Conceptual)</h2>
<p>Recall the variables: <span class="math inline">\(Y_1\)</span>=MATHPRE, <span class="math inline">\(Y_2\)</span>=MATHPOST, <span class="math inline">\(Y_3\)</span>=STANREAD, <span class="math inline">\(Y_4\)</span>=FRLUNCH (binary). Assume <span class="math inline">\(Y_2, Y_3, Y_4\)</span> have missing data.</p>
<p>The sequence of models at iteration <span class="math inline">\(t\)</span> might look like (using probit for <span class="math inline">\(Y_4\)</span> via latent <span class="math inline">\(Y_4^*\)</span>):</p>
<ol type="1">
<li><p><strong>Impute MATHPOST (<span class="math inline">\(Y_2\)</span>):</strong> <span class="math inline">\(Y_{2i}^{(t)} = \gamma_{02} + \gamma_{12}Y_{1i} + \gamma_{22}Y_{3i}^{(t-1)} + \gamma_{32}Y_{4i}^{(t-1)} + r_{2i}\)</span> <span class="math inline">\(Y_{2i(mis)}^{(t)} \sim N(\dots)\)</span></p></li>
<li><p><strong>Impute STANREAD (<span class="math inline">\(Y_3\)</span>):</strong> <span class="math inline">\(Y_{3i}^{(t)} = \gamma_{03} + \gamma_{13}Y_{1i} + \gamma_{23}Y_{4i}^{(t-1)} + \gamma_{33}Y_{2i}^{(t)} + r_{3i}\)</span> <span class="math inline">\(Y_{3i(mis)}^{(t)} \sim N(\dots)\)</span></p></li>
<li><p><strong>Impute FRLUNCH (<span class="math inline">\(Y_4\)</span> via <span class="math inline">\(Y_4^*\)</span>):</strong> <span class="math inline">\(Y_{4i}^{*(t)} = \gamma_{04} + \gamma_{14}Y_{1i} + \gamma_{24}Y_{2i}^{(t)} + \gamma_{34}Y_{3i}^{(t)} + r_{4i}\)</span> <span class="math inline">\(Y_{4i(mis)}^{*(t)} \sim N(\dots, 1)\)</span>; then determine <span class="math inline">\(Y_{4i(mis)}^{(t)}\)</span> based on threshold.</p></li>
</ol>
</section>
<section id="compatibility-in-fcs" class="slide level2">
<h2>Compatibility in FCS</h2>
</section>
<section id="the-compatibility-question" class="slide level2">
<h2>The Compatibility Question</h2>
<ul>
<li>An important theoretical issue with Fully Conditional Specification (FCS) is whether the specified sequence of conditional regression models is mutually <strong>compatible</strong></li>
<li><strong>Definition:</strong> Compatibility means the conditional distributions used for imputation (e.g., <span class="math inline">\(p(Y_1|Y_2, Y_3)\)</span>, <span class="math inline">\(p(Y_2|Y_1, Y_3)\)</span>, <span class="math inline">\(p(Y_3|Y_1, Y_2)\)</span>) are mathematically coherent</li>
<li><strong>Essence:</strong> Could these specified conditional distributions theoretically arise from a single, valid, underlying <em>joint distribution</em> <span class="math inline">\(p(Y_1, Y_2, Y_3)\)</span>?</li>
</ul>
</section>
<section id="example-bivariate-normal-compatibility" class="slide level2">
<h2>Example: Bivariate Normal Compatibility</h2>
<p>Consider two variables <span class="math inline">\((Y_1, Y_2)\)</span> following a bivariate normal distribution: <span class="math display">\[
\begin{pmatrix} Y_{1i} \\ Y_{2i} \end{pmatrix} \sim N_2 \left( \begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix}, \begin{pmatrix} \sigma_1^2 &amp; \sigma_{12} \\ \sigma_{21} &amp; \sigma_2^2 \end{pmatrix} \right)
\]</span> This <em>joint</em> distribution induces two <em>compatible</em> conditional linear regressions:</p>
<ol type="1">
<li><span class="math inline">\(Y_{1i} = \beta_0 + \beta_1 Y_{2i} + \epsilon_i \implies Y_{1i} \sim N_1(E(Y_{1i}|Y_{2i}), \sigma_\epsilon^2)\)</span></li>
<li><span class="math inline">\(Y_{2i} = \gamma_0 + \gamma_1 Y_{1i} + r_i \implies Y_{2i} \sim N_1(E(Y_{2i}|Y_{1i}), \sigma_r^2)\)</span></li>
</ol>
<p>They are compatible because the parameters (<span class="math inline">\(\beta\)</span>, <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\sigma^2\)</span>) are all functions of the <em>same</em> underlying joint distribution parameters (<span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(\sigma_{12}\)</span>)</p>
</section>
<section id="why-compatibility-matters-ideally" class="slide level2">
<h2>Why Compatibility Matters (Ideally)</h2>
<ul>
<li>Using a set of compatible conditional regression models for imputation is theoretically optimal</li>
<li>The resulting imputations should be logically consistent with each other, assuming the specified models are otherwise correctly specified for the data</li>
</ul>
</section>
<section id="practical-considerations-warnings" class="slide level2">
<h2>Practical Considerations &amp; Warnings</h2>
<ul>
<li><strong>Not All-or-Nothing:</strong> Perfect theoretical compatibility isn’t always required for good performance
<ul>
<li>Example: Using a logistic regression for a binary variable alongside linear regressions for continuous variables is technically incompatible (no joint distribution yields both), but often works well in practice and may differ little from using a compatible probit model</li>
</ul></li>
<li><strong>Major Warning - Nonlinearities:</strong> Significant incompatibilities <em>can</em> arise when applying standard FCS (using simple conditional regressions) to analysis models containing <strong>interactive</strong> or <strong>nonlinear</strong> terms (e.g., <span class="math inline">\(X \times M\)</span>, <span class="math inline">\(X^2\)</span>)
<ul>
<li>This often leads to biased estimates of the interaction/nonlinear effects</li>
<li><strong>Model-based imputation</strong> is generally preferred in these situations</li>
</ul></li>
</ul>
</section>
<section id="saving-filled-in-data-sets-in-fcs" class="slide level2">
<h2>Saving Filled-In Data Sets in FCS</h2>
<ul>
<li><p><strong>Goal:</strong> FCS uses Bayesian estimation for its conditional regression models, but the primary goal is typically saving the filled-in datasets for later analysis, not interpreting the imputation model parameters directly</p></li>
<li><p><strong>MCMC Algorithm per Variable:</strong> For each incomplete variable <span class="math inline">\(v\)</span> in the sequence, MCMC involves steps like:</p>
<ol type="1">
<li>Estimate regression coefficients (<span class="math inline">\(\boldsymbol{\gamma}_v\)</span>) given current residual variance and data</li>
<li>Estimate residual variance (<span class="math inline">\(\sigma^2_{r_v}\)</span>) given current coefficients and data</li>
<li>Update missing values for variable <span class="math inline">\(v\)</span> (including latent scores if used) based on the current parameters (<span class="math inline">\(\boldsymbol{\gamma}_v, \sigma^2_{r_v}\)</span>) <em>(+ estimate thresholds for ordinal variables)</em></li>
</ol></li>
</ul>
</section>
<section id="pooling-parameter-estimates" class="slide level2">
<h2>Pooling Parameter Estimates</h2>
</section>
<section id="the-basic-pooling-rule" class="slide level2">
<h2>The Basic Pooling Rule</h2>
<ul>
<li><strong>Starting Point:</strong> Analyzing <span class="math inline">\(M\)</span> imputed datasets yields <span class="math inline">\(M\)</span> separate estimates for each parameter of interest (e.g., <span class="math inline">\(\hat{\theta}_1, \hat{\theta}_2, ..., \hat{\theta}_M\)</span>)</li>
<li><strong>Combining Estimates:</strong> Rubin’s (1987) rule for the final pooled point estimate (<span class="math inline">\(\hat{\theta}\)</span>) is the simple arithmetic average of the <span class="math inline">\(M\)</span> estimates: <span class="math display">\[
  \hat{\theta} = \frac{1}{M} \sum_{m=1}^{M} \hat{\theta}_m
  \]</span></li>
</ul>
</section>
<section id="underlying-assumption" class="slide level2">
<h2>Underlying Assumption</h2>
<ul>
<li><strong>Normality:</strong> The formal statistical rationale for the simple averaging rule assumes that the parameter estimate of interest (the estimand) has an approximately <strong>normal sampling distribution</strong></li>
<li><strong>Common Estimates:</strong> This assumption holds reasonably well for many common statistics:
<ul>
<li>Means</li>
<li>Regression coefficients (slopes, intercepts)</li>
<li>Proportions</li>
</ul></li>
</ul>
</section>
<section id="handling-non-normal-estimands" class="slide level2">
<h2>Handling Non-Normal Estimands</h2>
<ul>
<li><strong>Issue:</strong> Some estimates do <em>not</em> typically follow a normal sampling distribution, especially in smaller samples. Examples include:
<ul>
<li>Correlation coefficients (<span class="math inline">\(r\)</span>)</li>
<li>Variances (<span class="math inline">\(\sigma^2\)</span>) and Standard Deviations (<span class="math inline">\(\sigma\)</span>)</li>
<li><span class="math inline">\(R^2\)</span> statistics</li>
<li>Odds Ratios (OR)</li>
</ul></li>
<li><strong>Problem:</strong> Directly averaging these estimates might not be optimal</li>
</ul>
</section>
<section id="strategy-for-non-normal-estimands" class="slide level2">
<h2>Strategy for Non-Normal Estimands</h2>
<ol type="1">
<li><strong>Transform:</strong> Apply a variance-stabilizing transformation to each of the <span class="math inline">\(M\)</span> estimates to make their distribution closer to normal
<ul>
<li><em>Example:</em> For correlations (<span class="math inline">\(r\)</span>), use Fisher’s z-transform: <span class="math inline">\(z = 0.5 \ln \left( \frac{1+r}{1-r} \right)\)</span></li>
</ul></li>
<li><strong>Pool:</strong> Calculate the arithmetic average of the <strong>transformed</strong> estimates (<span class="math inline">\(\bar{z}\)</span>)</li>
<li><strong>Back-Transform:</strong> Apply the inverse transformation to the pooled value to get the final estimate back on the original scale
<ul>
<li><em>Example:</em> For correlations, back-transform <span class="math inline">\(\bar{z}\)</span> using <span class="math inline">\(r = \tanh(\bar{z}) = \frac{e^{2\bar{z}}-1}{e^{2\bar{z}}+1}\)</span></li>
</ul></li>
</ol>
<ul>
<li><strong>Practical Impact:</strong> Transformations typically make a noticeable difference only in very small samples (e.g., <span class="math inline">\(N &lt; 50\)</span>)</li>
</ul>
</section>
<section id="what-not-to-pool-by-averaging" class="slide level2">
<h2>What NOT to Pool by Averaging</h2>
<ul>
<li><strong>Do NOT average:</strong>
<ul>
<li>Test statistics (e.g., t-statistics, F-statistics, <span class="math inline">\(\chi^2\)</span> statistics)</li>
<li>p-values</li>
</ul></li>
<li><strong>Reason:</strong> These quantities do not estimate a fixed population parameter and averaging them is statistically inappropriate. Specific pooling rules exist for standard errors and significance tests (covered next)</li>
</ul>
</section>
<section id="pooling-standard-errors" class="slide level2">
<h2>Pooling Standard Errors</h2>
</section>
<section id="the-problem-with-averaging-standard-errors" class="slide level2">
<h2>The Problem with Averaging Standard Errors</h2>
<ul>
<li>Analyzing <span class="math inline">\(M\)</span> imputed datasets also yields <span class="math inline">\(M\)</span> standard errors (<span class="math inline">\(SE_1, ..., SE_M\)</span>)</li>
<li><strong>Incorrect Approach:</strong> Simply averaging these standard errors (<span class="math inline">\(SE_m\)</span>) is <strong>wrong</strong></li>
<li><strong>Reason:</strong> Each <span class="math inline">\(SE_m\)</span> is calculated from a <em>complete</em> dataset, treating imputed values as if they were real observed values. This underestimates the true uncertainty because it ignores the uncertainty introduced by the imputation process itself.</li>
</ul>
</section>
<section id="two-components-of-variance" class="slide level2">
<h2>Two Components of Variance</h2>
<p>Rubin’s rules for standard errors correctly combine two sources of variance:</p>
<ol type="1">
<li><strong>Within-Imputation Variance (<span class="math inline">\(\bar{V}_W\)</span>)</strong>:
<ul>
<li>Represents the average sampling variance <em>within</em> each imputed dataset</li>
<li>Reflects the uncertainty we would expect if the data had been complete</li>
</ul></li>
<li><strong>Between-Imputation Variance (<span class="math inline">\(V_B\)</span>)</strong>:
<ul>
<li>Represents the variance <em>between</em> the parameter estimates across the <span class="math inline">\(M\)</span> imputed datasets</li>
<li>Reflects the extra uncertainty specifically due to the <strong>missing data</strong> and the imputation process</li>
</ul></li>
</ol>
</section>
<section id="within-imputation-variance-barv_w" class="slide level2">
<h2>Within-Imputation Variance (<span class="math inline">\(\bar{V}_W\)</span>)</h2>
<ul>
<li>This is the average of the squared standard errors (variances) from each of the <span class="math inline">\(M\)</span> analyses: <span class="math display">\[
  \overline{V}_W = \frac{1}{M} \sum_{m=1}^{M} SE_m^2
  \]</span> where <span class="math inline">\(SE_m^2\)</span> is the squared standard error of the estimate <span class="math inline">\(\hat{\theta}_m\)</span> from imputed dataset <span class="math inline">\(m\)</span></li>
<li><span class="math inline">\(\sqrt{\bar{V}_W}\)</span> represents the estimated standard error <em>ignoring</em> imputation uncertainty</li>
</ul>
</section>
<section id="between-imputation-variance-v_b" class="slide level2">
<h2>Between-Imputation Variance (<span class="math inline">\(V_B\)</span>)</h2>
<ul>
<li>This is the sample variance of the <span class="math inline">\(M\)</span> point estimates (<span class="math inline">\(\hat{\theta}_m\)</span>) around their pooled mean (<span class="math inline">\(\hat{\theta}\)</span>): <span class="math display">\[
  V_B = \frac{1}{M-1} \sum_{m=1}^{M} (\hat{\theta}_m - \hat{\theta})^2
  \]</span></li>
<li><span class="math inline">\(V_B\)</span> directly quantifies the variability introduced by the missing data – if there were no missing data (or if all imputations were identical), <span class="math inline">\(V_B\)</span> would be 0</li>
<li><strong>Crucial:</strong> Estimating <span class="math inline">\(V_B\)</span> requires more than one imputed dataset (<span class="math inline">\(M&gt;1\)</span>). This is why single imputation methods yield incorrect standard errors</li>
</ul>
</section>
<section id="total-variance-v_t" class="slide level2">
<h2>Total Variance (<span class="math inline">\(V_T\)</span>)</h2>
<ul>
<li>The total variance combines the within- and between-imputation components: <span class="math display">\[
  V_T = \bar{V}_W + V_B + \frac{V_B}{M}
  \]</span></li>
<li><span class="math inline">\(\bar{V}_W\)</span>: Average complete-data sampling variance</li>
<li><span class="math inline">\(V_B\)</span>: Added variance due to missing data</li>
<li><span class="math inline">\(V_B / M\)</span>: A small correction factor accounting for the fact that we used a finite number (<span class="math inline">\(M\)</span>) of imputations instead of infinite imputations. It represents the Monte Carlo error in estimating <span class="math inline">\(\hat{\theta}\)</span></li>
</ul>
</section>
<section id="pooled-standard-error-se" class="slide level2">
<h2>Pooled Standard Error (<span class="math inline">\(SE\)</span>)</h2>
<ul>
<li>The final pooled standard error for the parameter estimate <span class="math inline">\(\hat{\theta}\)</span> is simply the square root of the total variance: <span class="math display">\[
  SE = \sqrt{V_T} = \sqrt{\bar{V}_W + V_B + \frac{V_B}{M}}
  \]</span></li>
<li><strong>Result:</strong> Because <span class="math inline">\(V_B\)</span> (and <span class="math inline">\(V_B/M\)</span>) are non-negative, the pooled <span class="math inline">\(SE\)</span> is generally larger than the average standard error from the individual datasets (<span class="math inline">\(\approx \sqrt{\overline{V}_W}\)</span>), correctly reflecting the increased uncertainty due to missing information</li>
<li><strong>Note:</strong> MI standard errors are sometimes found to be slightly conservative (too large), but resulting confidence intervals and tests are often considered reliable</li>
</ul>
</section>
<section id="lavaan-example-see-r-syntax" class="slide level2">
<h2>Lavaan Example: See R syntax</h2>
</section>
<section class="slide level2">

<h3 id="riv-and-fmi-quantifying-missing-data-impact">RIV and FMI: Quantifying Missing Data Impact</h3>
</section>
<section id="purpose-of-riv-and-fmi" class="slide level2">
<h2>Purpose of RIV and FMI</h2>
<ul>
<li><strong>Goal:</strong> To express the impact of missing data on the precision of an estimate in proportional terms</li>
<li><strong>Basis:</strong> These two ratios repackage the variance components (<span class="math inline">\(\bar{V}_W\)</span>, <span class="math inline">\(V_B\)</span>, <span class="math inline">\(V_T\)</span>) from the standard error calculation</li>
<li><strong>Reporting:</strong> Often reported by software alongside pooled estimates and standard errors</li>
</ul>
</section>
<section id="relative-increase-in-variance-riv" class="slide level2">
<h2>Relative Increase in Variance (RIV)</h2>
<ul>
<li><strong>Compares:</strong> Missing data uncertainty (<span class="math inline">\(V_B\)</span>, adjusted for finite <span class="math inline">\(M\)</span>) relative to the complete-data sampling variance (<span class="math inline">\(\bar{V}_W\)</span>)</li>
<li><strong>Formula:</strong> <span class="math display">\[
  RIV = \frac{V_B + \frac{V_B}{M}}{\bar{V}_W}
  \]</span></li>
<li><strong>Interpretation:</strong> The proportional increase in variance (squared standard error) due to missing data, compared to the variance expected if data were complete
<ul>
<li>Minimum value is 0 (when <span class="math inline">\(V_B = 0\)</span>, i.e., no missing data)</li>
<li>Example: <span class="math inline">\(RIV = 0.12\)</span> means missing data inflated the variance by 12%</li>
</ul></li>
</ul>
</section>
<section id="fraction-of-missing-information-fmi" class="slide level2">
<h2>Fraction of Missing Information (FMI)</h2>
<ul>
<li><strong>Compares:</strong> Missing data uncertainty (<span class="math inline">\(V_B\)</span>, adjusted for finite <span class="math inline">\(M\)</span>) relative to the <strong>total</strong> variance (<span class="math inline">\(V_T\)</span>)</li>
<li><strong>Concept:</strong> Represents the proportion of the total variance in the estimate that is attributable to the missing data. Akin to an <span class="math inline">\(R^2\)</span> statistic for the “contribution” of missingness to the standard error</li>
</ul>
</section>
<section id="fmi-formulas" class="slide level2">
<h2>FMI: Formulas</h2>
<ul>
<li><strong>Approximation (large <span class="math inline">\(M\)</span>):</strong> <span class="math display">\[
  FMI \approx \frac{V_B + \frac{V_B}{M}}{V_T}
  \]</span></li>
<li><strong>Finite-<span class="math inline">\(M\)</span> Adjusted Formula:</strong> <span class="math display">\[
  FMI = \frac{V_B + \frac{V_B}{M}}{V_T} \times \frac{df_R+1}{df_R+3} + \frac{2}{df_R+3}
  \]</span> where <span class="math inline">\(df_R = (M-1)\left(1 + \frac{1}{RIV^2}\right)\)</span> are Rubin’s classic degrees of freedom for the estimate</li>
</ul>
</section>
<section id="fmi-interpretation-caveats" class="slide level2">
<h2>FMI: Interpretation &amp; Caveats</h2>
<ul>
<li><strong>Interpretation:</strong> Example: <span class="math inline">\(FMI = 0.11\)</span> indicates that about 11% of the total variance (squared standard error) of the parameter estimate is due to missing information</li>
<li><strong>FMI vs.&nbsp;% Missing:</strong> FMI is <strong>not</strong> the same as the percentage of cases with missing data
<ul>
<li>FMI can be lower (if variables are highly correlated, allowing good prediction) or sometimes even higher than the missing data rate</li>
</ul></li>
<li><strong>Estimation:</strong> Obtaining stable and accurate estimates of FMI often requires a <strong>large number of imputations</strong> (<span class="math inline">\(M\)</span>), potentially more than needed just for adequate power</li>
</ul>
</section>
<section id="test-statistic-confidence-intervals" class="slide level2">
<h2>Test Statistic &amp; Confidence Intervals</h2>
</section>
<section id="significance-testing-the-t-statistic" class="slide level2">
<h2>Significance Testing: The t-statistic</h2>
<ul>
<li>The familiar <strong>t-statistic</strong> provides a straightforward way to test hypotheses about a pooled parameter estimate (<span class="math inline">\(\hat{\theta}\)</span>)</li>
<li><strong>Formula:</strong> <span class="math display">\[
  t = \frac{\hat{\theta} - \theta_0}{SE}
  \]</span>
<ul>
<li><span class="math inline">\(\hat{\theta}\)</span> : Pooled point estimate</li>
<li><span class="math inline">\(SE\)</span>: Pooled standard error</li>
<li><span class="math inline">\(\theta_0\)</span>: Hypothesized parameter value under the null (usually 0)</li>
</ul></li>
</ul>
</section>
<section id="degrees-of-freedom-df-rubins-original-df_r" class="slide level2">
<h2>Degrees of Freedom (df): Rubin’s Original (<span class="math inline">\(df_R\)</span>)</h2>
<ul>
<li>The calculated t-statistic is compared to a t-distribution. Rubin (1987) proposed degrees of freedom based on the number of imputations (<span class="math inline">\(M\)</span>) and the Relative Increase in Variance (RIV): <span class="math display">\[
  df_R = (M-1)\left(1 + \frac{1}{RIV^2}\right)
  \]</span> where <span class="math inline">\(RIV = (V_B + V_B/M) / \overline{V}_W\)</span></li>
<li><strong>Issue:</strong> <span class="math inline">\(df_R\)</span> can sometimes exceed the sample size <span class="math inline">\(N\)</span> or the complete-data degrees of freedom (<span class="math inline">\(df_{com}\)</span>), which is counter-intuitive. For large <span class="math inline">\(df_R\)</span>, the t-test is effectively a z-test.</li>
</ul>
</section>
<section id="degrees-of-freedom-df-barnard-rubin-df_br" class="slide level2">
<h2>Degrees of Freedom (df): Barnard &amp; Rubin (<span class="math inline">\(df_{BR}\)</span>)</h2>
<ul>
<li>Barnard &amp; Rubin (1999) proposed an adjusted degrees of freedom (<span class="math inline">\(df_{BR}\)</span>) to address issues with <span class="math inline">\(df_R\)</span></li>
<li><strong>Concept:</strong> Combines <span class="math inline">\(df_R\)</span> with an estimate of the “observed-data” degrees of freedom (<span class="math inline">\(df_{obs}\)</span>), which adjusts the complete-data df (<span class="math inline">\(df_{com}\)</span>) based on the Fraction of Missing Information (FMI)</li>
<li><strong>Formula Sketch:</strong> <span class="math display">\[
  df_{BR} = \frac{df_R \cdot df_{obs}}{df_R + df_{obs}} \quad \text{where } df_{obs} \approx f(df_{com}, FMI)
  \]</span></li>
<li><strong>Properties:</strong> <span class="math inline">\(df_{BR}\)</span> decreases as missing information increases (as expected) and <span class="math inline">\(df_{BR} \le df_{com}\)</span>. Recommended especially for smaller samples</li>
</ul>
</section>
<section id="reference-distributions-t-vs.-z" class="slide level2">
<h2>Reference Distributions: t vs.&nbsp;z</h2>
<ul>
<li><strong>t-distribution:</strong> Use the calculated t-statistic with either <span class="math inline">\(df_R\)</span> or (preferably for smaller samples) <span class="math inline">\(df_{BR}\)</span></li>
<li><strong>z-distribution (Standard Normal):</strong> Some software might simply treat the test statistic <span class="math inline">\(t = (\hat{\theta} - \theta_0) / SE\)</span> as a z-score</li>
<li><strong>Practical Impact:</strong> The choice of df adjustment (<span class="math inline">\(df_R\)</span> vs <span class="math inline">\(df_{BR}\)</span>) or reference distribution (t vs.&nbsp;z) usually makes little difference to conclusions unless the sample size is very small</li>
</ul>
</section>
<section id="confidence-intervals-ci" class="slide level2">
<h2>Confidence Intervals (CI)</h2>
<ul>
<li>Calculate the confidence interval using the pooled estimate (<span class="math inline">\(\hat{\theta}\)</span>), the pooled standard error (<span class="math inline">\(SE\)</span>), and the appropriate critical value (<span class="math inline">\(t_{CV}\)</span>) from the t-distribution</li>
<li><strong>Formula:</strong> <span class="math display">\[
  CI = \hat{\theta} \pm t_{CV} \times SE
  \]</span></li>
<li><strong>Getting <span class="math inline">\(t_{CV}\)</span>:</strong> Find the critical value for the desired alpha level (e.g., <span class="math inline">\(\alpha=0.05\)</span> for 95% CI) using the chosen degrees of freedom (<span class="math inline">\(df_R\)</span> or <span class="math inline">\(df_{BR}\)</span>)</li>
</ul>
</section>
<section id="important-note-on-cis" class="slide level2">
<h2>Important Note on CIs</h2>
<ul>
<li><strong>DO NOT</strong> calculate confidence intervals for each of the <span class="math inline">\(M\)</span> imputed datasets and then average the interval limits
<ul>
<li>This is incorrect because the individual standard errors (<span class="math inline">\(SE_m\)</span>) used to create those CIs are too small (they ignore imputation uncertainty)</li>
</ul></li>
<li><strong>CORRECT Approach:</strong> Always construct the final confidence interval using the <strong>pooled estimate (<span class="math inline">\(\hat{\theta}\)</span>)</strong>, the <strong>pooled standard error (<span class="math inline">\(SE\)</span>)</strong>, and the <strong>appropriate degrees of freedom (<span class="math inline">\(df_R\)</span> or <span class="math inline">\(df_{BR}\)</span>)</strong> to find <span class="math inline">\(t_{CV}\)</span></li>
<li><strong>Inference:</strong> Check if the null value <span class="math inline">\(\theta_0\)</span> falls outside the calculated confidence interval</li>
</ul>
</section>
<section id="when-might-mi-give-different-answers" class="slide level2">
<h2>When Might MI Give Different Answers?</h2>
</section>
<section id="recap-mi-vs.-direct-estimation-mlbayesian" class="slide level2">
<h2>Recap: MI vs.&nbsp;Direct Estimation (ML/Bayesian)</h2>
<ul>
<li><strong>Direct Estimation (ML &amp; Bayesian):</strong> Single-stage procedures that estimate model parameters directly from the observed data</li>
<li><strong>Multiple Imputation (MI):</strong> Two-stage procedure:
<ol type="1">
<li>Imputation Stage: Create completed datasets using an imputation model</li>
<li>Analysis Stage: Analyze the completed datasets using the analysis model(s) and pool results</li>
</ol></li>
<li><strong>Question:</strong> When will MI results align with, or differ from, ML/Bayesian results?</li>
</ul>
</section>
<section id="key-factor-model-mismatch-congeniality" class="slide level2">
<h2>Key Factor: Model (Mis)match &amp; Congeniality</h2>
<ul>
<li><strong>The Difference Lies:</strong> Primarily in the potential mismatch between the imputation model and the analysis model, especially regarding:
<ul>
<li>Variables included in each model</li>
<li>Assumptions and structure imposed by each model</li>
</ul></li>
<li><strong>Congeniality (Meng, 1994):</strong> Models are “congenial” when the imputation model and analysis model align appropriately</li>
</ul>
</section>
<section id="congenial-models-proposition-1" class="slide level2">
<h2>Congenial Models: Proposition 1</h2>
<ul>
<li><strong>Scenario:</strong> MI and direct estimation use the <strong>same set of variables</strong> AND invoke models with <strong>equivalent assumptions and structure</strong></li>
<li><strong>Result:</strong> Direct estimation and MI will be <strong>equivalent</strong></li>
<li><strong>Example:</strong> Using a bivariate normal imputation model for pre/post scores when the analysis is a paired t-test (which assumes normally distributed differences, a consequence of bivariate normality)</li>
</ul>
</section>
<section id="congenial-models-proposition-2" class="slide level2">
<h2>Congenial Models: Proposition 2</h2>
<ul>
<li><strong>Scenario:</strong> MI and direct estimation use the <strong>same set of variables</strong>, BUT the imputation model is <strong>less restrictive</strong> (uses more parameters) than the analysis model</li>
<li><strong>Example:</strong> Using a saturated imputation model (unrestricted means, variances, covariances) before fitting a constrained structural equation model (e.g., CFA)</li>
<li><strong>Result:</strong> Parameter estimates will be <strong>equivalent</strong>, but MI standard errors <em>might</em> be slightly larger due to the less restrictive imputation model</li>
</ul>
</section>
<section id="uncongenial-models-proposition-3" class="slide level2">
<h2>Uncongenial Models: Proposition 3</h2>
<ul>
<li><strong>Scenario:</strong> The imputation stage and the direct estimation analysis use <strong>different sets of variables</strong>. This leads to “uncongenial” models (two examples described next)</li>
</ul>
</section>
<section id="uncongenial-case-excluding-analysis-variables" class="slide level2">
<h2>Uncongenial Case: Excluding Analysis Variables</h2>
<ul>
<li><strong>Action:</strong> Excluding a variable needed for the final analysis model from the imputation model</li>
<li><strong>Result:</strong> Generally <strong>detrimental</strong>. It implicitly assumes the correlation between the omitted variable and the imputed values is zero. Parameter estimates involving the omitted variable will typically be <strong>biased towards zero</strong> (unless the true association is zero)</li>
<li><strong>Rule:</strong> Always include all analysis variables in the imputation model</li>
</ul>
</section>
<section id="uncongenial-case-including-auxiliary-variables" class="slide level2">
<h2>Uncongenial Case: Including Auxiliary Variables</h2>
<ul>
<li><strong>Action:</strong> Including extra “auxiliary variables” in the imputation model that are <em>not</em> part of the final analysis model</li>
<li><strong>Result:</strong> Usually considered <strong>beneficial</strong>. Auxiliary variables (correlated with analysis variables or missingness) can:
<ul>
<li>Reduce nonresponse bias</li>
<li>Improve statistical power</li>
</ul></li>
<li><strong>MI vs.&nbsp;Direct Estimation:</strong> In this case, MI (with aux vars) can give different (and potentially better) estimates and standard errors than direct estimation (without aux vars). Differences are most apparent with high missing data rates (e.g., &gt; 25%)</li>
</ul>
</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
</section>
<section id="mi-recap-imputation-first" class="slide level2">
<h2>MI Recap: Imputation First</h2>
<ul>
<li><strong>Contrast:</strong> Unlike ML and Bayesian estimation (which estimate parameters directly), Multiple Imputation (MI) focuses first on <strong>creating suitable imputations</strong> for later analysis</li>
<li><strong>Process:</strong> MI typically involves 3 steps:
<ol type="1">
<li>Create <span class="math inline">\(M\)</span> filled-in datasets using an imputation model</li>
<li>Perform the desired analysis on each of the <span class="math inline">\(M\)</span> datasets</li>
<li>Pool the results (estimates, standard errors) using Rubin’s rules</li>
</ol></li>
<li><strong>Equivalence:</strong> Under the same variable set and assumptions, MI generally yields results indistinguishable from ML or Bayesian estimation</li>
</ul>
</section>
<section id="agnostic-vs.-model-based-imputation-recap" class="slide level2">
<h2>Agnostic vs.&nbsp;Model-Based Imputation (Recap)</h2>
<p>The chapter classified MI procedures based on the match between imputation and analysis models</p>
<ul>
<li><strong>Agnostic Imputation:</strong>
<ul>
<li>Imputation model differs from the substantive analysis model (e.g., uses a general joint distribution)</li>
<li>Well-suited for analyses <em>without</em> nonlinear effects (interactions, polynomials, random effects)</li>
</ul></li>
<li><strong>Model-Based Imputation:</strong>
<ul>
<li>Imputation model <em>is</em> the substantive analysis model (potentially plus auxiliary variables)</li>
<li>Generally the ideal approach for analyses <em>with</em> nonlinear effects</li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<table class="caption-top">
<colgroup>
<col style="width: 15%">
<col style="width: 27%">
<col style="width: 26%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Maximum Likelihood (ML)</th>
<th style="text-align: left;">Bayesian Estimation</th>
<th style="text-align: left;">Multiple Imputation (MI)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Primary Goal</strong></td>
<td style="text-align: left;">Fit model to observed data</td>
<td style="text-align: left;">Fit model to observed data</td>
<td style="text-align: left;">Create suitable imputed datasets for analysis</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Missing Data Handling</strong></td>
<td style="text-align: left;">Implicitly integrated via likelihood</td>
<td style="text-align: left;">Implicitly integrated via MCMC/priors</td>
<td style="text-align: left;">Explicitly handled in separate first stage</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Process</strong></td>
<td style="text-align: left;">Single stage</td>
<td style="text-align: left;">Single stage</td>
<td style="text-align: left;">Two stages (Impute, Analyze/Pool)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Focus</strong></td>
<td style="text-align: left;">Parameter estimates from observed data</td>
<td style="text-align: left;">Parameter distributions from observed data</td>
<td style="text-align: left;">Filled-in datasets</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Output</strong></td>
<td style="text-align: left;">Parameter estimates &amp; SEs</td>
<td style="text-align: left;">Posterior distributions for parameters</td>
<td style="text-align: left;"><span class="math inline">\(M\)</span> datasets; Pooled estimates &amp; SEs</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Auxiliary Variables</strong></td>
<td style="text-align: left;">Include in analysis model</td>
<td style="text-align: left;">Include in analysis/predictor models</td>
<td style="text-align: left;">Easily include in imputation model</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Nonlinearities</strong></td>
<td style="text-align: left;">Requires specific model specification</td>
<td style="text-align: left;">Requires specific model specification</td>
<td style="text-align: left;">May require Model-Based MI</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Software</strong></td>
<td style="text-align: left;">Widely available (e.g., lavaan FIML)</td>
<td style="text-align: left;">Widely available (e.g., JAGS, Stan)</td>
<td style="text-align: left;">Widely available (e.g., mice, SAS)</td>
</tr>
</tbody>
</table>
</section>
<section id="mi-vs.-ml-vs.-bayesian-approaches" class="slide level2">
<h2>MI vs.&nbsp;ML vs.&nbsp;Bayesian Approaches</h2>
<ul>
<li><strong>Key Distinction:</strong> ML and Bayesian methods estimate parameters <em>directly</em> from the observed data, handling missingness implicitly within the model estimation. MI <em>separates</em> the handling of missing data (imputation stage) from the substantive analysis (analysis/pooling stages)</li>
<li><strong>Equivalence:</strong> Often yield similar results when using the same variables and assumptions. MI’s flexibility with auxiliary variables can sometimes be an advantage</li>
</ul>

</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p><a href="https://jonathantemplin.github.io/MissingDataMethods2025/">https://jonathantemplin.github.io/MissingDataMethods2025/</a></p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="08_Multiple_Imputation_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="08_Multiple_Imputation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="08_Multiple_Imputation_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="08_Multiple_Imputation_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="08_Multiple_Imputation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="08_Multiple_Imputation_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="08_Multiple_Imputation_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="08_Multiple_Imputation_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="08_Multiple_Imputation_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="08_Multiple_Imputation_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>