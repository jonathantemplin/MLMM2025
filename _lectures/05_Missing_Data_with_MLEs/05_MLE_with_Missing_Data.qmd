---
title: "Maximum Likelihood Estimation with Missing Data"
author: "Jonathan Templin"
date: "March 5, 2025"
format: html
---

# Introduction

This document demonstrates Maximum Likelihood Estimation (MLE) in the presence of missing data, following Chapter 3 of Enders (2022). The examples utilize the `lavaan` package for structural equation modeling (SEM) and demonstrate the handling of missing data using FIML (Full Information Maximum Likelihood).

# Loading Required Packages

We begin by loading necessary packages. If they are not installed, the script will install them first.

```{r}
if (!require(mvtnorm)) install.packages("mvtnorm"); library(mvtnorm)
if (!require(lavaan)) install.packages("lavaan"); library(lavaan)
if (!require(semPlot)) install.packages("semPlot"); library(semPlot)
```

## Simulating Data for the Example

In this section, we generate simulated data for the analysis. The data consist of two groups: men and women, with predefined means, correlations, and standard deviations for a set of variables. We then combine these datasets and prepare them for further analysis.

### Setting the Seed
Setting a seed ensures reproducibility so that the same random numbers are generated each time the script is run.

```{r}
set.seed(20250223)
```

### Defining Sample Sizes
We define the sample sizes for the two groups:

```{r}
Nmen = 121
Nwomen = 229
```

### Defining Mean Vectors
The mean vectors for men and women are specified for the seven variables:

```{r}
MeanMEN = c(5.081333347, 9.723030258, 51.94404048, 52.66452548, 34.04606078, 77.06181845, 14.75389904)
MeanWOMEN = c(4.80418631, 10.60486174, 50.34834542, 48.13359134, 30.61321679, 71.77082955, 13.75449003)
```

These vectors contain the expected means for each variable in the dataset.

### Defining the Correlation Matrix
We specify the correlations among the seven variables:

```{r}
Corr = matrix(c(
  1.0,  .15, .12, .48, .44, .47, .44,
  .15, 1.0, .06, .25, .20, .23, .23,
  .12, .06, 1.0, .40, .32, .19, .14,
  .48, .25, .40, 1.0, .87, .61, .54,
  .44, .20, .32, .87, 1.0, .56, .51,
  .47, .23, .19, .61, .56, 1.0, .70,
  .44, .23, .14, .54, .51, .70, 1.0
), nrow = 7, byrow = TRUE)
```

### Defining Standard Deviations and Constructing the Covariance Matrix
The standard deviations are stored in a vector and used to compute the covariance matrix:

```{r}
SD = c(1.2, 6.0, 15.2, 16.6, 10.9, 10.5, 2.8)
SDdiag = diag(SD)

# Create the covariance matrix
Cov = SDdiag %*% Corr %*% SDdiag
```

### Generating Multivariate Normal Data
Using the `rmvnorm` function, we generate normally distributed data for both groups:

```{r}
xmen = rmvnorm(Nmen, MeanMEN, Cov)
xwomen = rmvnorm(Nwomen, MeanWOMEN, Cov)
```

### Adding Group Identifiers
To distinguish between men and women, we append a group identifier:

```{r}
xmen = cbind(0, xmen)  # 1 for men
xwomen = cbind(1, xwomen)  # 0 for women
```

### Combining and Formatting the Dataset
The two datasets are combined into a single data frame, and column names are assigned:

```{r}
allx = as.data.frame(rbind(xmen, xwomen))
names(allx) = c("female", "hsl", "cc", "use", "msc", "mas", "mse", "perf")
```

At this point, we have a dataset with 350 observations (121 men and 229 women), each with values on seven variables. The variable `female` acts as a binary indicator (0 = male, 1 = female).

## Introducing Missingness in Key Variables

This section introduces missing data into three variables (`cc`, `perf`, and `use`) based on other variables in the dataset. The probability of missingness is modeled using logistic regression equations that depend on `female`, `msc`, `mas`, and `mse`.

### Introducing Missingness in `cc`
We first define a logistic regression model for missingness in `cc`, using coefficients that determine how each predictor influences the probability of missing data.

```{r}
M_CC_intercept = -3
M_CC_female = 1
M_CC_msc = .01
M_CC_mas = .01
M_CC_mse = .01

missingCCmodelLogit = 
  M_CC_intercept +
  M_CC_female*allx[,"female"] +
  M_CC_msc*allx[,"msc"] +
  M_CC_mas*allx[,"mas"] +
  M_CC_mse*allx[,"mse"]

missingCCmodelProb = exp(missingCCmodelLogit)/(1+exp(missingCCmodelLogit))
```

Using these probabilities, we introduce missing values in `cc`:

```{r}
makeMissingCC = which(runif(Nmen+Nwomen) < missingCCmodelProb)
allx$cc[makeMissingCC] = NA
allx$cc
```

### Introducing Missingness in `perf`
Next, we define a logistic model for missingness in `perf`:

```{r}
M_perf_intercept = -3
M_perf_female = .5
M_perf_msc = .001
M_perf_mas = .02
M_perf_mse = .01

missingPERFmodelLogit = 
  M_perf_intercept +
  M_perf_female*allx[,"female"] +
  M_perf_msc*allx[,"msc"] +
  M_perf_mas*allx[,"mas"] +
  M_perf_mse*allx[,"mse"]

missingPERFmodelProb = exp(missingPERFmodelLogit)/(1+exp(missingPERFmodelLogit))
```

We then apply this model to introduce missing values in `perf`:

```{r}
makeMissingPerf = which(runif(Nmen+Nwomen) < missingPERFmodelProb)
allx$perf[makeMissingPerf] = NA
allx$perf
```

### Introducing Missingness in `use`
Finally, we define and apply the missing data model for `use`:

```{r}
M_use_intercept = -3
M_use_female = .5
M_use_msc = .001
M_use_mas = .02
M_use_mse = .01

missingUSEmodelLogit = 
  M_use_intercept +
  M_use_female*allx[,"female"] +
  M_use_msc*allx[,"msc"] +
  M_use_mas*allx[,"mas"] +
  M_use_mse*allx[,"mse"]

missingUSEmodelProb = exp(missingUSEmodelLogit)/(1+exp(missingUSEmodelLogit))
```

Applying the model to introduce missing values in `use`:

```{r}
makeMissingUse = which(runif(Nmen+Nwomen) < missingUSEmodelProb)
allx$use[makeMissingUse] = NA
allx$use
```

### Converting the Data into a Data Frame
Finally, we store the modified dataset in a new data frame for further analysis:

```{r}
data02 = as.data.frame(allx)
```

## Centering `cc` and Initial Data Exploration

To improve interpretability, we center the `cc` variable at 10 and create an interaction term between `female` and `cc10`.

### Visualizing `cc`
We first generate a boxplot and a histogram to examine the distribution of `cc` before centering:

```{r}
par(mfrow = c(1,2))
boxplot(data02$cc, main="Boxplot of College Experience (CC)")
hist(data02$cc, main = "Histogram of College Experience (CC)", xlab = "CC")
par(mfrow = c(1,1))
```

### Centering `cc`

```{r}
data02$cc10 = data02$cc - 10
```

### Creating an Interaction Term

```{r}
data02$femXcc10 = data02$female*data02$cc10
```

### Preparing Data for Analysis
We create a subset of the dataset including only relevant variables:

```{r}
newData = data02[c("perf", "use", "female", "cc10")]
```

### Checking for Completely Missing Rows

```{r}
allNA = apply(newData, 1, function(x) all(is.na(x)))
which(allNA)
```

No observations are missing all variables, so we proceed.

## Building Analysis Model #1: Empty Model

This model estimates the means, variances, and covariance of `perf` and `use` without predictors.

### Checking Missing Data Patterns

```{r}
sum(is.na(newData$perf) & is.na(newData$use))
sum(!is.na(newData$perf) | !is.na(newData$use))
```

### Defining the Model Syntax

```{r}
model01.syntax = "

#Means:
perf ~ 1
use  ~ 1

#Variances:
perf ~~ perf
use  ~~ use

#Covariance:
perf ~~ use
"
```

### Model Estimation Using `lavaan`

```{r}
model01.fit = lavaan(model01.syntax, data=data02, estimator = "MLR", mimic = "MPLUS")
summary(model01.fit, standardized=TRUE, fit.measures=TRUE)
```

### Comparing Model-Implied and Observed Covariance Matrices

```{r}
fitted(model01.fit)

meanVec = colMeans(data02[c("perf", "use")], na.rm = TRUE)
meanVec

cov(data02[c("perf", "use")], use = "complete.obs")

N = sum(!is.na(newData$perf) | !is.na(newData$use))
covMat = cov(data02[c("perf", "use")], use = "complete.obs")*(N-1)/N
covMat
```

### Inspecting Model Residuals

```{r}
inspect(model01.fit, what="sampstat.h1")
residuals(model01.fit, type = "raw")
residuals(model01.fit, type = "normalized")
```

### Checking R-Squared Values

```{r}
inspect(model01.fit, what="r2")
```

### Visualizing the Model
We generate path diagrams to visualize the estimated relationships.

```{r}
semPaths(model01.fit, intercepts = FALSE, residuals = TRUE, style="mx", layout="tree", rotation=2,
         optimizeLatRes = TRUE, sizeMan = 8, what ="path")

semPaths(model01.fit, intercepts = FALSE, residuals = TRUE, style="mx", layout="tree", rotation=2,
         optimizeLatRes = TRUE, sizeMan = 8, whatLabels ="est")
```

## Verifying Log-Likelihood Calculation

In this section, we compare the log-likelihood calculated manually with the one obtained from `lavaan` to confirm that they match.

### Manual Log-Likelihood Calculation
We first extract complete cases and compute the log-likelihood manually:

```{r}
completeData = data02[, c("perf", "use")]

# remove cases that are missing on all variables
completeData = completeData[!is.na(completeData$perf) | !is.na(completeData$use),]
logLike = rep(NA, nrow(completeData))
for (obs in 1:nrow(completeData)){
  obsData = completeData[obs,]
  obsVars = names(completeData)[which(!is.na(obsData))]
  logLike[obs] = dmvnorm(
    x = obsData[obsVars], 
    mean = meanVec[obsVars], 
    sigma = covMat[obsVars, obsVars, drop=FALSE], 
    log = TRUE
  )
}

# our calculation
sum(logLike)
```

### Log-Likelihood from `lavaan`

```{r}
sum(inspect(model01.fit, what="loglik.casewise"), na.rm = TRUE)
```

## Comparing Factored Regression Specifications
We now compare different factored regression specifications to confirm they yield the same log-likelihood.

### Model 1b: `perf ~ use` and `use ~ 1`

```{r}
model01b.syntax = "

# exogenous mean:
use ~ 1

# exogenous variance:
use ~~ use

# regression:
perf ~ 1 + use

# residual variance:
perf ~~ perf
"

model01b.fit = lavaan(model01b.syntax, data=data02, estimator = "MLR", mimic = "MPLUS")
summary(model01b.fit, standardized=TRUE, fit.measures=TRUE)
```

### Model 1c: `use ~ perf` and `perf ~ 1`

```{r}
model01c.syntax = "

# exogenous mean:
perf ~ 1

# exogenous variance:
perf ~~ perf

# regression:
use ~ 1 + perf

# residual variance:
use ~~ use
"

model01c.fit = lavaan(model01c.syntax, data=data02, estimator = "MLR", mimic = "MPLUS")
summary(model01c.fit, standardized=TRUE, fit.measures=TRUE)
```

## Model 2: Adding Predictors as Main Effects

In this section, we expand our analysis by adding predictors (`female` and `cc10`) as main effects in the regression models for `perf` and `use`.

### Specifying the Model Syntax
We define a structural equation model where `perf` and `use` are regressed on `female` and `cc10`. The variances and covariances of `perf` and `use` are also estimated.

```{r}
model02.syntax = "

#Means:
perf ~ 1 + female + cc10
use  ~ 1 + female + cc10

#Variances:
perf ~~ perf
use  ~~ use

#Covariance:
perf ~~ use
"
```

### Estimating the Model
We fit the model using the `lavaan` package with the `MLR` estimator and `MPLUS` mimic settings.

```{r}
model02.fit = lavaan(model02.syntax, data=data02, estimator = "MLR", mimic = "MPLUS")
```

### Summarizing the Model Results
We generate a summary of the model fit, including standardized estimates and fit indices.

```{r}
summary(model02.fit, standardized=TRUE, fit.measures=TRUE)
```

### Inspecting Model-Implied Mean Vector and Covariance Matrix
The `fitted()` function provides the expected mean vector and covariance matrix under the estimated model.

```{r}
fitted(model02.fit)
```

### Examining the Saturated Model Statistics
The `inspect()` function allows us to see the sample statistics from the fully saturated model.

```{r}
inspect(model02.fit, what="sampstat.h1")
```

### Assessing Residuals
We check the raw residuals to assess the discrepancy between model-implied and observed covariance structures.

```{r}
residuals(model02.fit, type = "raw")
```

We also examine the normalized residuals to further evaluate model fit.

```{r}
residuals(model02.fit, type = "normalized")
```

### Checking Modification Indices
The `modindices()` function provides information about potential model improvements by suggesting additional parameters.

```{r}
modindices(model02.fit)
```

### Calculating R-Squared Values for Dependent Variables
We inspect the proportion of variance explained for the dependent variables in the model.

```{r}
inspect(model02.fit, what="r2")
```

### Visualizing the Model
Finally, we create path diagrams to illustrate the relationships in the estimated model.

#### Path Diagram Without Estimates

```{r}
semPaths(model02.fit, intercepts = FALSE, residuals = TRUE, style="mx", layout="tree", rotation=2,
         optimizeLatRes = TRUE, sizeMan = 8, what ="path")
```

#### Path Diagram With Estimates

```{r}
semPaths(model02.fit, intercepts = FALSE, residuals = TRUE, style="mx", layout="tree", rotation=2,
         optimizeLatRes = TRUE, sizeMan = 8, whatLabels ="est")
```


## Model 3: Expanding the Likelihood Function

In this section, we expand the likelihood function by incorporating more variables into the estimation process.

### Specifying the Model Syntax
This model includes `cc10` as an exogenous variable and estimates its mean and variance. We also specify regressions for `perf` and `use` with `female` and `cc10` as predictors, while maintaining variance and covariance structures.

```{r}
model03.syntax = "
# Exogenous Variables 
#Mean:
cc10 ~ 1

#Variance:
cc10 ~~ cc10

# Endogenous Variables 
#Regressions:
perf ~ 1 + female + cc10
use  ~ 1 + female + cc10

#Residual Variances:
perf ~~ perf
use  ~~ use

#Residual Covariance:
perf ~~ use
"
```

### Estimating the Model
We fit the expanded model using `lavaan`, ensuring that there are no warnings related to missing data.

```{r}
model03.fit = lavaan(model03.syntax, data=data02, estimator = "MLR", mimic = "MPLUS")
```

### Summarizing the Model Results
We extract and examine the model summary, including standardized estimates and fit measures.

```{r}
summary(model03.fit, standardized=TRUE, fit.measures=TRUE)
```

### Inspecting Model-Implied Mean Vector and Covariance Matrix
We check the expected mean vector and covariance matrix under the estimated model.

```{r}
fitted(model03.fit)
```

### Examining the Saturated Model Statistics
We inspect the sample statistics from the fully saturated model.

```{r}
inspect(model03.fit, what="sampstat.h1")
```

### Assessing Residuals
We evaluate the raw residuals to assess model fit discrepancies.

```{r}
residuals(model03.fit, type = "raw")
```

We also check the normalized residuals for further model diagnostics.

```{r}
residuals(model03.fit, type = "normalized")
```

### Checking Modification Indices
The `modindices()` function suggests potential model improvements by identifying additional parameters to estimate.

```{r}
modindices(model03.fit)
```

### Calculating R-Squared Values for Dependent Variables
We compute the proportion of variance explained for the dependent variables in the model.

```{r}
inspect(model03.fit, what="r2")
```

### Visualizing the Model
We generate path diagrams to illustrate the estimated relationships.

#### Path Diagram Without Estimates

```{r}
semPaths(model03.fit, intercepts = FALSE, residuals = TRUE, style="mx", layout="tree", rotation=2,
         optimizeLatRes = TRUE, sizeMan = 8, what ="path")
```

#### Path Diagram With Estimates

```{r}
semPaths(model03.fit, intercepts = FALSE, residuals = TRUE, style="mx", layout="tree", rotation=2,
         optimizeLatRes = TRUE, sizeMan = 8, whatLabels ="est")
```

## Handling Missing Data in a Categorical Variable (`female`)
We introduce missingness in `female` completely at random (MCAR) to assess its impact on model estimation.

### Creating Missing Values in `female`
We randomly introduce missing values in 10% of observations for `female`.

```{r}
data03 = data02
data03$female[sample(x = 1:nrow(data03), replace = TRUE, size = .1*nrow(data03) )] = NA
data03$female
```

### Running the Model with Missing `female`
We attempt to estimate `model03.syntax` on the modified dataset, expecting a warning about missing data since `female` is not explicitly included in the likelihood function.

```{r}
model03b.fit = lavaan(model03.syntax, data=data03, estimator = "MLR", mimic = "MPLUS")
```

## Model 4: Adding `female` to the Likelihood Function

In this section, we extend the likelihood function to explicitly model `female` as an exogenous variable alongside `cc10`.

### Specifying the Model Syntax
This model includes `female` as an exogenous variable, estimating its mean and variance, as well as its covariance with `cc10`. The endogenous variables `perf` and `use` are regressed on both `female` and `cc10`.

```{r}
model04.syntax = "

# Exogenous Variables 
#Mean:
cc10 ~ 1
female ~ 1

#Variance:
cc10 ~~ cc10
female ~~ female

#Covariance:
cc10 ~~ female

# Endogenous Variables 
#Regressions:
perf ~ 1 + female + cc10
use  ~ 1 + female + cc10

#Residual Variances:
perf ~~ perf
use  ~~ use

#Residual Covariance:
perf ~~ use
"
```

### Estimating the Model
We fit the model using `lavaan`, incorporating `female` into the likelihood function.

```{r}
model04.fit = lavaan(model04.syntax, data=data03, estimator = "MLR", mimic = "MPLUS")
```

### Summarizing the Model Results
We extract and examine the model summary, including standardized estimates and fit measures.

```{r}
summary(model04.fit, standardized=TRUE, fit.measures=TRUE)
```

### Inspecting Model-Implied Mean Vector and Covariance Matrix
We check the expected mean vector and covariance matrix under the estimated model.

```{r}
fitted(model04.fit)
```

### Examining the Saturated Model Statistics
We inspect the sample statistics from the fully saturated model.

```{r}
inspect(model04.fit, what="sampstat.h1")
```

### Assessing Residuals
We evaluate the raw residuals to assess model fit discrepancies.

```{r}
residuals(model04.fit, type = "raw")
```

We also check the normalized residuals for further model diagnostics.

```{r}
residuals(model04.fit, type = "normalized")
```

### Checking Modification Indices
The `modindices()` function suggests potential model improvements by identifying additional parameters to estimate.

```{r}
modindices(model04.fit)
```

### Calculating R-Squared Values for Dependent Variables
We compute the proportion of variance explained for the dependent variables in the model.

```{r}
inspect(model04.fit, what="r2")
```

### Visualizing the Model
We generate path diagrams to illustrate the estimated relationships.

#### Path Diagram Without Estimates

```{r}
semPaths(model04.fit, intercepts = FALSE, residuals = TRUE, style="mx", layout="tree", rotation=2,
         optimizeLatRes = TRUE, sizeMan = 8, what ="path")
```

#### Path Diagram With Estimates

```{r}
semPaths(model04.fit, intercepts = FALSE, residuals = TRUE, style="mx", layout="tree", rotation=2,
         optimizeLatRes = TRUE, sizeMan = 8, whatLabels ="est")
```

### Addressing the Normality Issue with `female`
Since `female` is not normally distributed, we will need to use factored regression and the EM algorithm to correct for this issue in subsequent models.

## Handling Non-Normal `female` Using Factored Regression and the EM Algorithm

In this section, we use factored regression and the Expectation-Maximization (EM) algorithm to handle the non-normal distribution of `female`. The factorization follows:

$$f(use, perf, cc10, female) = f(use|perf, cc10, female) * f(perf|cc10, female) * f(cc10|female) * f(female)$$

### Initializing Model Parameters
We set initial values for the regression coefficients in the factored regression framework.

```{r}
# f(female):
female_intercept = 0

# f(cc10|female) 
cc10_intercept = 0
cc10_female = 1

# f(perf|cc10, female):
perf_intercept = 0
perf_cc10 = 1
perf_female = 1

# f(use|perf, cc10, female):
use_intercept = 0
use_perf = 1
use_cc10 = 1
use_female = 1
```

### Identifying Missing Data
We determine which observations have missing values in each variable.

```{r}
femaleMissing = which(is.na(data03$female))
cc10Missing = which(is.na(data03$cc10))
perfMissing = which(is.na(data03$perf))
useMissing = which(is.na(data03$use))
```

### Setting Up the EM Algorithm
We create a working copy of the dataset and set the number of iterations.

```{r}
algorithmData = data03
maxIterations = 10
iteration = 1
```

### EM Algorithm: Expectation and Maximization Steps
We iterate through the EM process, imputing missing data in the E-step and re-estimating parameters in the M-step.

```{r}
while(iteration < maxIterations){
  # E-step: Impute missing values
  algorithmData$female[femaleMissing] = 
    round(exp(female_intercept)/(1+exp(female_intercept)), 0)
  
  algorithmData$cc10[cc10Missing] = 
    cc10_intercept + 
    cc10_female*algorithmData$female[cc10Missing]
  
  algorithmData$perf[perfMissing] =
    perf_intercept + 
    perf_cc10*algorithmData$cc10[perfMissing] + 
    perf_female*algorithmData$female[perfMissing]
  
  algorithmData$use[useMissing] =
    use_intercept + 
    use_perf*algorithmData$perf[useMissing] + 
    use_cc10*algorithmData$cc10[useMissing] +
    use_female*algorithmData$female[useMissing]
  
  # M-step: Update regression coefficients
  femaleModel = glm(female ~ 1, data = algorithmData, family = binomial(link="logit"))
  female_intercept = femaleModel$coefficients[1]
  logLikeFemale = dbinom(
    x = algorithmData$female, 
    size = 1, 
    prob = exp(female_intercept)/(1+exp(female_intercept)), 
    log = TRUE
  )
  
  cc10Model = lm(cc10 ~ female, data = algorithmData)
  cc10_intercept = cc10Model$coefficients["(Intercept)"]
  cc10_female = cc10Model$coefficients["female"]
  cc10_residVar = anova(cc10Model)$`Mean Sq`[2]
  logLikeCC10 = dnorm(
    x = algorithmData$cc10, 
    mean = cc10_intercept + cc10_female*algorithmData$female,
    sd = sqrt(cc10_residVar),
    log = TRUE
  )
  
  perfModel = lm(perf ~ cc10 + female, data = algorithmData)
  perf_intercept = perfModel$coefficients["(Intercept)"]
  perf_cc10 = perfModel$coefficients["cc10"]
  perf_female = perfModel$coefficients["female"]
  perf_residVar = anova(perfModel)$`Mean Sq`[2]
  logLikePerf = dnorm(
    x = algorithmData$perf, 
    mean = perf_intercept + perf_cc10*algorithmData$cc10 + perf_female*algorithmData$female,
    sd = sqrt(perf_residVar),
    log = TRUE
  )
  
  useModel = lm(use ~ perf + cc10 + female, data = algorithmData)
  use_intercept = useModel$coefficients["(Intercept)"]
  use_perf = useModel$coefficients["perf"]
  use_cc10 = useModel$coefficients["cc10"]
  use_female = useModel$coefficients["female"]
  use_residVar = anova(useModel)$`Mean Sq`[2]
  logLikeUse = dnorm(
    x = algorithmData$use, 
    mean = use_intercept + use_perf*algorithmData$perf + use_cc10*algorithmData$cc10 + use_female*algorithmData$female,
    sd = sqrt(use_residVar),
    log = TRUE
  )
  
  # Compute total log-likelihood
  logLikeMatrix = cbind(
    logLikeFemale,
    logLikeCC10,
    logLikePerf,
    logLikeUse
  )
  loglik = sum(rowSums(logLikeMatrix))
  
  # Print iteration number and log-likelihood
  cat("Iteration: ", iteration, "Loglikelihood: ", loglik, "\n")
  iteration = iteration + 1
}
```

### Summarizing the Final Models
After running the EM algorithm, we summarize the estimated models.

```{r}
summary(femaleModel)
summary(cc10Model)
summary(perfModel)
summary(useModel)
```

## Adding Interactions Using the Just-Another-Variable Approach

In this section, we introduce an interaction term between `female` and `cc10` within the SEM framework. The interaction is treated as a separate variable, allowing `lavaan` to estimate its mean, variance, and covariances.

### Creating the Interaction Variable
We first compute the interaction term as the product of `female` and `cc10`.

```{r}
data03$femXcc10 = data03$female * data03$cc10
```

### Specifying the Model Syntax
This model includes `femXcc10` as an exogenous variable, alongside `cc10` and `female`. The model estimates means, variances, and covariances for these variables, as well as their effects on `perf` and `use`.

```{r}
model05.syntax = "

# Exogenous Variables 
#Mean:
cc10 ~ 1
female ~ 1
femXcc10 ~ 1

#Variances:
cc10 ~~ cc10
female ~~ female
femXcc10 ~~ femXcc10

#Covariances:
cc10 ~~ female
cc10 ~~ femXcc10
female ~~ femXcc10

# Endogenous Variables 
#Regressions:
perf ~ 1 + female + cc10 + femXcc10
use  ~ 1 + female + cc10 + femXcc10

#Residual Variances:
perf ~~ perf
use  ~~ use

#Residual Covariance:
perf ~~ use
"
```

### Estimating the Model
We fit the model using `lavaan`, incorporating the interaction term as an exogenous predictor.

```{r}
model05.fit = lavaan(model05.syntax, data=data03, estimator = "MLR", mimic = "MPLUS")
```

### Summarizing the Model Results
We extract and examine the model summary, including standardized estimates and fit measures.

```{r}
summary(model05.fit, standardized=TRUE, fit.measures=TRUE)
```

### Inspecting Model-Implied Mean Vector and Covariance Matrix
We check the expected mean vector and covariance matrix under the estimated model.

```{r}
fitted(model05.fit)
```

### Examining the Saturated Model Statistics
We inspect the sample statistics from the fully saturated model.

```{r}
inspect(model05.fit, what="sampstat.h1")
```

### Assessing Residuals
We evaluate the raw residuals to assess model fit discrepancies.

```{r}
residuals(model05.fit, type = "raw")
```

We also check the normalized residuals for further model diagnostics.

```{r}
residuals(model05.fit, type = "normalized")
```

### Checking Modification Indices
The `modindices()` function suggests potential model improvements by identifying additional parameters to estimate.

```{r}
modindices(model05.fit)
```

### Calculating R-Squared Values for Dependent Variables
We compute the proportion of variance explained for the dependent variables in the model.

```{r}
inspect(model05.fit, what="r2")
```

### Visualizing the Model
We generate path diagrams to illustrate the estimated relationships.

#### Path Diagram Without Estimates

```{r}
semPaths(model05.fit, intercepts = FALSE, residuals = TRUE, style="mx", layout="tree", rotation=2,
         optimizeLatRes = TRUE, sizeMan = 8, what ="path")
```

#### Path Diagram With Estimates

```{r}
semPaths(model05.fit, intercepts = FALSE, residuals = TRUE, style="mx", layout="tree", rotation=2,
         optimizeLatRes = TRUE, sizeMan = 8, whatLabels ="est")
```

## Factored Regression with Interaction Terms Using the EM Algorithm

In this section, we use factored regression and the Expectation-Maximization (EM) algorithm to estimate missing values while incorporating an interaction term (`femXcc10`). The model follows:

\[ f(use, perf, cc10, female, femXcc10) = f(use|perf, cc10, female, femXcc10) * f(perf|cc10, female, femXcc10) * f(cc10|female) * f(female) \]

### Initializing Model Parameters
We set initial values for the regression coefficients in the factored regression framework.

```{r}
# f(female):
female_intercept = 0

# f(cc10|female) 
cc10_intercept = 0
cc10_female = 1

# f(perf|cc10, female):
perf_intercept = 0
perf_cc10 = 1
perf_female = 1
perf_femXcc10 = 1

# f(use|perf, cc10, female, femXcc10):
use_intercept = 0
use_perf = 1
use_cc10 = 1
use_female = 1
use_femXcc10 = 1
```

### Identifying Missing Data
We determine which observations have missing values in each variable.

```{r}
femaleMissing = which(is.na(data03$female))
cc10Missing = which(is.na(data03$cc10))
perfMissing = which(is.na(data03$perf))
useMissing = which(is.na(data03$use))
```

### Setting Up the EM Algorithm
We create a working copy of the dataset and set the number of iterations.

```{r}
algorithmData = data03
maxIterations = 10
iteration = 1
```

### EM Algorithm: Expectation and Maximization Steps
We iterate through the EM process, imputing missing data in the E-step and re-estimating parameters in the M-step.

```{r}
while(iteration < maxIterations){
  # E-step: Impute missing values
  algorithmData$female[femaleMissing] = 
    round(exp(female_intercept)/(1+exp(female_intercept)), 0)
  
  algorithmData$cc10[cc10Missing] = 
    cc10_intercept + 
    cc10_female*algorithmData$female[cc10Missing]
  
  algorithmData$perf[perfMissing] =
    perf_intercept + 
    perf_cc10*algorithmData$cc10[perfMissing] + 
    perf_female*algorithmData$female[perfMissing] +
    perf_femXcc10*algorithmData$cc10[perfMissing]*algorithmData$female[perfMissing]
  
  algorithmData$use[useMissing] =
    use_intercept + 
    use_perf*algorithmData$perf[useMissing] + 
    use_cc10*algorithmData$cc10[useMissing] +
    use_female*algorithmData$female[useMissing] +
    use_femXcc10*algorithmData$cc10[useMissing]*algorithmData$female[useMissing]
  
  # M-step: Update regression coefficients
  femaleModel = glm(female ~ 1, data = algorithmData, family = binomial(link="logit"))
  female_intercept = femaleModel$coefficients[1]
  
  cc10Model = lm(cc10 ~ female, data = algorithmData)
  cc10_intercept = cc10Model$coefficients["(Intercept)"]
  cc10_female = cc10Model$coefficients["female"]
  
  perfModel = lm(perf ~ cc10 + female + cc10:female, data = algorithmData)
  perf_intercept = perfModel$coefficients["(Intercept)"]
  perf_cc10 = perfModel$coefficients["cc10"]
  perf_female = perfModel$coefficients["female"]
  perf_femXcc10 = perfModel$coefficients["cc10:female"]
  
  useModel = lm(use ~ perf + cc10 + female + cc10:female, data = algorithmData)
  use_intercept = useModel$coefficients["(Intercept)"]
  use_perf = useModel$coefficients["perf"]
  use_cc10 = useModel$coefficients["cc10"]
  use_female = useModel$coefficients["female"]
  use_femXcc10 = useModel$coefficients["cc10:female"]
  
  # Compute total log-likelihood
  loglik = sum(
    dbinom(algorithmData$female, size = 1, prob = exp(female_intercept)/(1+exp(female_intercept)), log = TRUE) +
    dnorm(algorithmData$cc10, mean = cc10_intercept + cc10_female*algorithmData$female, sd = sqrt(var(algorithmData$cc10)), log = TRUE) +
    dnorm(algorithmData$perf, mean = perf_intercept + perf_cc10*algorithmData$cc10 + perf_female*algorithmData$female + perf_femXcc10*algorithmData$cc10*algorithmData$female, sd = sqrt(var(algorithmData$perf)), log = TRUE) +
    dnorm(algorithmData$use, mean = use_intercept + use_perf*algorithmData$perf + use_cc10*algorithmData$cc10 + use_female*algorithmData$female + use_femXcc10*algorithmData$cc10*algorithmData$female, sd = sqrt(var(algorithmData$use)), log = TRUE)
  )
  
  # Print iteration number and log-likelihood
  cat("Iteration: ", iteration, "Loglikelihood: ", loglik, "\n")
  iteration = iteration + 1
}
```

### Summarizing the Final Models
After running the EM algorithm, we summarize the estimated models.

```{r}
summary(femaleModel)
summary(cc10Model)
summary(perfModel)
summary(useModel)
```

## Incorporating Auxiliary Variables in the SEM Framework

In this section, we extend our SEM model by including auxiliary variables (`hsl`, `msc`, `mas`, `mse`). These variables help improve estimation by capturing additional variance and reducing bias due to missing data.

### Specifying the Model Syntax
We define a model that includes the auxiliary variables and estimates their means, variances, and covariances.

```{r}
model06.syntax = "

# Exogenous Variables (now with auxiliary variables)
#Means:
cc10 ~ 1
female ~ 1

# Auxiliary variables
hsl ~ 1
msc ~ 1
mas ~ 1
mse ~ 1

#Variances:
cc10 ~~ cc10
female ~~ female

# Auxiliary variables:
hsl ~~ hsl
msc ~~ msc
mas ~~ mas
mse ~~ mse

#Covariances:
cc10 ~~ female + hsl + msc + mas + mse
female ~~ hsl + msc + mas + mse
hsl ~~ msc + mas + mse
msc ~~ mas + mse
mas ~~ mse

# Endogenous Variables 
#Regressions:
perf ~ 1 + female + cc10
use  ~ 1 + female + cc10

#Residual Variances:
perf ~~ perf
use  ~~ use

#Residual Covariance:
perf ~~ use

#Endogenous/exogenous covariances
perf ~~ hsl + msc + mas + mse
use  ~~ hsl + msc + mas + mse
"
```

### Estimating the Model
We fit the model using `lavaan`, incorporating auxiliary variables into the SEM framework.

```{r}
model06.fit = lavaan(model06.syntax, data=data03, estimator = "MLR", mimic = "MPLUS")
```

### Summarizing the Model Results
We extract and examine the model summary, including standardized estimates and fit measures.

```{r}
summary(model06.fit, standardized=TRUE, fit.measures=TRUE)
```

### Inspecting Model-Implied Mean Vector and Covariance Matrix
We check the expected mean vector and covariance matrix under the estimated model.

```{r}
fitted(model06.fit)
```

### Examining the Saturated Model Statistics
We inspect the sample statistics from the fully saturated model.

```{r}
inspect(model06.fit, what="sampstat.h1")
```

### Assessing Residuals
We evaluate the raw residuals to assess model fit discrepancies.

```{r}
residuals(model06.fit, type = "raw")
```

We also check the normalized residuals for further model diagnostics.

```{r}
residuals(model06.fit, type = "normalized")
```

### Checking Modification Indices
The `modindices()` function suggests potential model improvements by identifying additional parameters to estimate.

```{r}
modindices(model06.fit)
```

### Calculating R-Squared Values for Dependent Variables
We compute the proportion of variance explained for the dependent variables in the model.

```{r}
inspect(model06.fit, what="r2")
```

### Visualizing the Model
We generate path diagrams to illustrate the estimated relationships.

#### Path Diagram Without Estimates

```{r}
semPaths(model06.fit, intercepts = FALSE, residuals = TRUE, style="mx", layout="tree", rotation=2,
         optimizeLatRes = TRUE, sizeMan = 8, what ="path")
```

#### Path Diagram With Estimates

```{r}
semPaths(model06.fit, intercepts = FALSE, residuals = TRUE, style="mx", layout="tree", rotation=2,
         optimizeLatRes = TRUE, sizeMan = 8, whatLabels ="est")
```

## Analysis Strategy Decision Strategy

This text-based flowchart outlines the decision-making process for handling missing data in analysis.

1. **Have missing data?**  
   - **No** → Conduct analysis as usual.  
   - **Yes** → Proceed to the next step.  
      - **Minimum goal:** Build an analysis that meets the assumptions of Missing at Random (MAR). For Maximum Likelihood Estimation, use likelihoods to your benefit.  

2. **Are all variables with missing data (in your model and auxiliary variables) plausibly continuous (so that you can assume they follow multivariate normal distributions)?**  
   - **Yes** → Use **SEM framework (lavaan)** where all variables (including auxiliary variables) are in the likelihood function.  
   - **No** → Use **factored regression approach** (likely not EM algorithm unless you are savvy at coding), as implemented in **Bayesian methods (next lecture)**. Or use **Multiple Imputation methods** that treat non-normal data appropriately.  

This structured approach ensures a systematic decision-making process for handling missing data in analysis.
